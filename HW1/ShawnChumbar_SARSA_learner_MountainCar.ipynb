{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 01 - HW on Gym\n",
        "CMPE 260 Homework 1\n",
        "By Shawn Chumbar\n",
        "\n",
        "## Assignment Description\n",
        "Use the Q-learning example given as a basis for your homework and develop a\n",
        "SARSA learning algorithm instead of Q-learning.\n"
      ],
      "metadata": {
        "id": "b60Tl4_ah3um"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "FMPQk_vRzSbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[atari]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_1iH-IyqdmB",
        "outputId": "feb12c27-8c2a-4087-9122-c9d3070a3d8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium[atari]\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[atari])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Collecting shimmy<1.0,>=0.1.0 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (6.4.5)\n",
            "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: farama-notifications, gymnasium, ale-py, shimmy\n",
            "Successfully installed ale-py-0.8.1 farama-notifications-0.0.4 gymnasium-0.29.1 shimmy-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"ale-py>=0.8.0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKrhyn3rqdiX",
        "outputId": "9ac023fe-9fe0-40ae-99b9-ead947edbdf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ale-py>=0.8.0 in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ale-py>=0.8.0) (1.26.4)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py>=0.8.0) (6.4.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py>=0.8.0) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1UQ1fh08Awj",
        "outputId": "58d4a440-b1ae-4e49-d384-2a31ab7f3547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autorom[accept-rom-license]\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (2.32.3)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (2024.8.30)\n",
            "Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446661 sha256=4587a0c38834593f96b438cadc3ede1ef46edb6e76bc697f43f990d48c9ccb3d\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install autorom[accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordVideo"
      ],
      "metadata": {
        "id": "aDVyfrCetPTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_gym_env(name, n):\n",
        "    env = gym.make(name, render_mode=\"rgb_array\")\n",
        "    env = RecordVideo(env, \"videos\")\n",
        "    observation, info = env.reset()\n",
        "    for _ in range(int(n)):\n",
        "        action = env.action_space.sample()\n",
        "        observation, reward, terminated, truncated, info = env.step(action)\n",
        "        if terminated or truncated:\n",
        "            observation, info = env.reset()\n",
        "    env.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_gym_env('ALE/Alien-v5', 2000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2Sq50dJA8wu",
        "outputId": "e6c283a3-8228-4141-ce86-9e36189745d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:335: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video /content/videos/rl-video-episode-0.mp4.\n",
            "Moviepy - Writing video /content/videos/rl-video-episode-0.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/rl-video-episode-0.mp4\n",
            "Moviepy - Building video /content/videos/rl-video-episode-1.mp4.\n",
            "Moviepy - Writing video /content/videos/rl-video-episode-1.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/rl-video-episode-1.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q_Learner\n",
        "The code below is the standard Q Learner class that we were given in Class."
      ],
      "metadata": {
        "id": "tShojJn2qi8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "MAX_NUM_EPISODES = 50000\n",
        "STEPS_PER_EPISODE = 200\n",
        "EPSILON_MIN = 0.005\n",
        "max_num_steps = MAX_NUM_EPISODES * STEPS_PER_EPISODE\n",
        "EPSILON_DECAY = 500 * EPSILON_MIN / max_num_steps\n",
        "ALPHA = 0.05\n",
        "GAMMA = 0.98\n",
        "NUM_DISCRETE_BINS = 30\n",
        "\n",
        "class Q_Learner(object):\n",
        "    def __init__(self, env):\n",
        "        self.obs_shape = env.observation_space.shape\n",
        "        self.obs_high = env.observation_space.high\n",
        "        self.obs_low = env.observation_space.low\n",
        "        self.obs_bins = NUM_DISCRETE_BINS\n",
        "        self.bin_width = (self.obs_high - self.obs_low) / self.obs_bins\n",
        "        self.action_shape = env.action_space.n\n",
        "        self.Q = np.zeros((self.obs_bins + 1, self.obs_bins + 1, self.action_shape))\n",
        "        self.alpha = ALPHA\n",
        "        self.gamma = GAMMA\n",
        "        self.epsilon = 1.0\n",
        "\n",
        "    def discretize(self, obs):\n",
        "        return tuple(((obs - self.obs_low) / self.bin_width).astype(int))\n",
        "\n",
        "    def get_action(self, obs):\n",
        "        discretized_obs = self.discretize(obs)\n",
        "        if self.epsilon > EPSILON_MIN:\n",
        "            self.epsilon -= EPSILON_DECAY\n",
        "        if np.random.random() > self.epsilon:\n",
        "            return np.argmax(self.Q[discretized_obs])\n",
        "        else:\n",
        "            return np.random.choice([a for a in range(self.action_shape)])\n",
        "\n",
        "    def learn(self, obs, action, reward, next_obs):\n",
        "        discretized_obs = self.discretize(obs)\n",
        "        discretized_next_obs = self.discretize(next_obs)\n",
        "        td_target = reward + self.gamma * np.max(self.Q[discretized_next_obs])\n",
        "        td_error = td_target - self.Q[discretized_obs][action]\n",
        "        self.Q[discretized_obs][action] += self.alpha * td_error\n",
        "\n",
        "def train(agent, env):\n",
        "    best_reward = -float('inf')\n",
        "    for episode in range(MAX_NUM_EPISODES):\n",
        "        obs, _ = env.reset()\n",
        "        total_reward = 0.0\n",
        "        for _ in range(STEPS_PER_EPISODE):\n",
        "            action = agent.get_action(obs)\n",
        "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            agent.learn(obs, action, reward, next_obs)\n",
        "            obs = next_obs\n",
        "            total_reward += reward\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "        if total_reward > best_reward:\n",
        "            best_reward = total_reward\n",
        "        if episode % 100 == 0:\n",
        "            print(f\"Episode#{episode} reward:{total_reward} best_reward:{best_reward} eps:{agent.epsilon:.3f}\")\n",
        "    return np.argmax(agent.Q, axis=2)\n",
        "\n",
        "def test(agent, env, policy):\n",
        "    obs, _ = env.reset()\n",
        "    total_reward = 0.0\n",
        "    for _ in range(STEPS_PER_EPISODE):\n",
        "        action = policy[agent.discretize(obs)]\n",
        "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "        obs = next_obs\n",
        "        total_reward += reward\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "    return total_reward\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = gym.make('MountainCar-v0')\n",
        "    agent = Q_Learner(env)\n",
        "    learned_policy = train(agent, env)\n",
        "\n",
        "    test_env = gym.make('MountainCar-v0', render_mode='rgb_array')\n",
        "    test_env = gym.wrappers.RecordVideo(test_env, \"videos\")\n",
        "    for _ in range(5):  # Test for 5 episodes\n",
        "        total_reward = test(agent, test_env, learned_policy)\n",
        "        print(f\"Test episode reward: {total_reward}\")\n",
        "    test_env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucmV7J_fBTj9",
        "outputId": "54be733a-e832-447b-c81a-e5caf7e8f129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode#0 reward:-200.0 best_reward:-200.0 eps:1.000\n",
            "Episode#100 reward:-200.0 best_reward:-200.0 eps:0.995\n",
            "Episode#200 reward:-200.0 best_reward:-200.0 eps:0.990\n",
            "Episode#300 reward:-200.0 best_reward:-200.0 eps:0.985\n",
            "Episode#400 reward:-200.0 best_reward:-200.0 eps:0.980\n",
            "Episode#500 reward:-200.0 best_reward:-200.0 eps:0.975\n",
            "Episode#600 reward:-200.0 best_reward:-200.0 eps:0.970\n",
            "Episode#700 reward:-200.0 best_reward:-200.0 eps:0.965\n",
            "Episode#800 reward:-200.0 best_reward:-200.0 eps:0.960\n",
            "Episode#900 reward:-200.0 best_reward:-200.0 eps:0.955\n",
            "Episode#1000 reward:-200.0 best_reward:-200.0 eps:0.950\n",
            "Episode#1100 reward:-200.0 best_reward:-200.0 eps:0.945\n",
            "Episode#1200 reward:-200.0 best_reward:-200.0 eps:0.940\n",
            "Episode#1300 reward:-200.0 best_reward:-200.0 eps:0.935\n",
            "Episode#1400 reward:-200.0 best_reward:-200.0 eps:0.930\n",
            "Episode#1500 reward:-200.0 best_reward:-200.0 eps:0.925\n",
            "Episode#1600 reward:-200.0 best_reward:-200.0 eps:0.920\n",
            "Episode#1700 reward:-200.0 best_reward:-200.0 eps:0.915\n",
            "Episode#1800 reward:-200.0 best_reward:-200.0 eps:0.910\n",
            "Episode#1900 reward:-200.0 best_reward:-200.0 eps:0.905\n",
            "Episode#2000 reward:-200.0 best_reward:-200.0 eps:0.900\n",
            "Episode#2100 reward:-200.0 best_reward:-200.0 eps:0.895\n",
            "Episode#2200 reward:-200.0 best_reward:-200.0 eps:0.890\n",
            "Episode#2300 reward:-200.0 best_reward:-200.0 eps:0.885\n",
            "Episode#2400 reward:-200.0 best_reward:-200.0 eps:0.880\n",
            "Episode#2500 reward:-200.0 best_reward:-200.0 eps:0.875\n",
            "Episode#2600 reward:-200.0 best_reward:-200.0 eps:0.870\n",
            "Episode#2700 reward:-200.0 best_reward:-200.0 eps:0.865\n",
            "Episode#2800 reward:-200.0 best_reward:-200.0 eps:0.860\n",
            "Episode#2900 reward:-200.0 best_reward:-200.0 eps:0.855\n",
            "Episode#3000 reward:-200.0 best_reward:-200.0 eps:0.850\n",
            "Episode#3100 reward:-200.0 best_reward:-200.0 eps:0.845\n",
            "Episode#3200 reward:-200.0 best_reward:-200.0 eps:0.840\n",
            "Episode#3300 reward:-200.0 best_reward:-200.0 eps:0.835\n",
            "Episode#3400 reward:-200.0 best_reward:-200.0 eps:0.830\n",
            "Episode#3500 reward:-200.0 best_reward:-200.0 eps:0.825\n",
            "Episode#3600 reward:-200.0 best_reward:-200.0 eps:0.820\n",
            "Episode#3700 reward:-200.0 best_reward:-200.0 eps:0.815\n",
            "Episode#3800 reward:-200.0 best_reward:-200.0 eps:0.810\n",
            "Episode#3900 reward:-200.0 best_reward:-200.0 eps:0.805\n",
            "Episode#4000 reward:-200.0 best_reward:-200.0 eps:0.800\n",
            "Episode#4100 reward:-200.0 best_reward:-200.0 eps:0.795\n",
            "Episode#4200 reward:-200.0 best_reward:-200.0 eps:0.790\n",
            "Episode#4300 reward:-200.0 best_reward:-200.0 eps:0.785\n",
            "Episode#4400 reward:-200.0 best_reward:-200.0 eps:0.780\n",
            "Episode#4500 reward:-200.0 best_reward:-200.0 eps:0.775\n",
            "Episode#4600 reward:-200.0 best_reward:-200.0 eps:0.770\n",
            "Episode#4700 reward:-200.0 best_reward:-200.0 eps:0.765\n",
            "Episode#4800 reward:-200.0 best_reward:-200.0 eps:0.760\n",
            "Episode#4900 reward:-200.0 best_reward:-200.0 eps:0.755\n",
            "Episode#5000 reward:-200.0 best_reward:-200.0 eps:0.750\n",
            "Episode#5100 reward:-200.0 best_reward:-200.0 eps:0.745\n",
            "Episode#5200 reward:-200.0 best_reward:-200.0 eps:0.740\n",
            "Episode#5300 reward:-200.0 best_reward:-200.0 eps:0.735\n",
            "Episode#5400 reward:-200.0 best_reward:-200.0 eps:0.730\n",
            "Episode#5500 reward:-200.0 best_reward:-200.0 eps:0.725\n",
            "Episode#5600 reward:-200.0 best_reward:-200.0 eps:0.720\n",
            "Episode#5700 reward:-200.0 best_reward:-200.0 eps:0.715\n",
            "Episode#5800 reward:-200.0 best_reward:-200.0 eps:0.710\n",
            "Episode#5900 reward:-200.0 best_reward:-200.0 eps:0.705\n",
            "Episode#6000 reward:-200.0 best_reward:-200.0 eps:0.700\n",
            "Episode#6100 reward:-200.0 best_reward:-200.0 eps:0.695\n",
            "Episode#6200 reward:-200.0 best_reward:-200.0 eps:0.690\n",
            "Episode#6300 reward:-200.0 best_reward:-200.0 eps:0.685\n",
            "Episode#6400 reward:-200.0 best_reward:-200.0 eps:0.680\n",
            "Episode#6500 reward:-200.0 best_reward:-200.0 eps:0.675\n",
            "Episode#6600 reward:-200.0 best_reward:-200.0 eps:0.670\n",
            "Episode#6700 reward:-200.0 best_reward:-200.0 eps:0.665\n",
            "Episode#6800 reward:-200.0 best_reward:-200.0 eps:0.660\n",
            "Episode#6900 reward:-200.0 best_reward:-200.0 eps:0.655\n",
            "Episode#7000 reward:-200.0 best_reward:-200.0 eps:0.650\n",
            "Episode#7100 reward:-200.0 best_reward:-200.0 eps:0.645\n",
            "Episode#7200 reward:-200.0 best_reward:-200.0 eps:0.640\n",
            "Episode#7300 reward:-200.0 best_reward:-200.0 eps:0.635\n",
            "Episode#7400 reward:-200.0 best_reward:-200.0 eps:0.630\n",
            "Episode#7500 reward:-200.0 best_reward:-200.0 eps:0.625\n",
            "Episode#7600 reward:-200.0 best_reward:-200.0 eps:0.620\n",
            "Episode#7700 reward:-200.0 best_reward:-200.0 eps:0.615\n",
            "Episode#7800 reward:-200.0 best_reward:-200.0 eps:0.610\n",
            "Episode#7900 reward:-200.0 best_reward:-200.0 eps:0.605\n",
            "Episode#8000 reward:-200.0 best_reward:-200.0 eps:0.600\n",
            "Episode#8100 reward:-200.0 best_reward:-200.0 eps:0.595\n",
            "Episode#8200 reward:-200.0 best_reward:-200.0 eps:0.590\n",
            "Episode#8300 reward:-200.0 best_reward:-199.0 eps:0.585\n",
            "Episode#8400 reward:-200.0 best_reward:-199.0 eps:0.580\n",
            "Episode#8500 reward:-200.0 best_reward:-199.0 eps:0.575\n",
            "Episode#8600 reward:-200.0 best_reward:-199.0 eps:0.570\n",
            "Episode#8700 reward:-200.0 best_reward:-199.0 eps:0.565\n",
            "Episode#8800 reward:-200.0 best_reward:-199.0 eps:0.560\n",
            "Episode#8900 reward:-200.0 best_reward:-168.0 eps:0.555\n",
            "Episode#9000 reward:-200.0 best_reward:-168.0 eps:0.550\n",
            "Episode#9100 reward:-200.0 best_reward:-168.0 eps:0.545\n",
            "Episode#9200 reward:-200.0 best_reward:-168.0 eps:0.540\n",
            "Episode#9300 reward:-200.0 best_reward:-168.0 eps:0.535\n",
            "Episode#9400 reward:-200.0 best_reward:-168.0 eps:0.530\n",
            "Episode#9500 reward:-200.0 best_reward:-168.0 eps:0.525\n",
            "Episode#9600 reward:-200.0 best_reward:-167.0 eps:0.520\n",
            "Episode#9700 reward:-200.0 best_reward:-167.0 eps:0.515\n",
            "Episode#9800 reward:-200.0 best_reward:-167.0 eps:0.510\n",
            "Episode#9900 reward:-200.0 best_reward:-163.0 eps:0.505\n",
            "Episode#10000 reward:-200.0 best_reward:-163.0 eps:0.500\n",
            "Episode#10100 reward:-200.0 best_reward:-154.0 eps:0.495\n",
            "Episode#10200 reward:-200.0 best_reward:-154.0 eps:0.490\n",
            "Episode#10300 reward:-200.0 best_reward:-154.0 eps:0.485\n",
            "Episode#10400 reward:-200.0 best_reward:-154.0 eps:0.480\n",
            "Episode#10500 reward:-200.0 best_reward:-154.0 eps:0.475\n",
            "Episode#10600 reward:-200.0 best_reward:-154.0 eps:0.470\n",
            "Episode#10700 reward:-200.0 best_reward:-154.0 eps:0.465\n",
            "Episode#10800 reward:-200.0 best_reward:-154.0 eps:0.460\n",
            "Episode#10900 reward:-200.0 best_reward:-154.0 eps:0.455\n",
            "Episode#11000 reward:-200.0 best_reward:-154.0 eps:0.450\n",
            "Episode#11100 reward:-200.0 best_reward:-154.0 eps:0.445\n",
            "Episode#11200 reward:-200.0 best_reward:-154.0 eps:0.440\n",
            "Episode#11300 reward:-200.0 best_reward:-154.0 eps:0.435\n",
            "Episode#11400 reward:-200.0 best_reward:-154.0 eps:0.431\n",
            "Episode#11500 reward:-200.0 best_reward:-154.0 eps:0.426\n",
            "Episode#11600 reward:-169.0 best_reward:-154.0 eps:0.421\n",
            "Episode#11700 reward:-200.0 best_reward:-154.0 eps:0.416\n",
            "Episode#11800 reward:-200.0 best_reward:-154.0 eps:0.411\n",
            "Episode#11900 reward:-200.0 best_reward:-154.0 eps:0.406\n",
            "Episode#12000 reward:-168.0 best_reward:-154.0 eps:0.401\n",
            "Episode#12100 reward:-200.0 best_reward:-154.0 eps:0.396\n",
            "Episode#12200 reward:-200.0 best_reward:-154.0 eps:0.391\n",
            "Episode#12300 reward:-200.0 best_reward:-154.0 eps:0.386\n",
            "Episode#12400 reward:-152.0 best_reward:-152.0 eps:0.381\n",
            "Episode#12500 reward:-200.0 best_reward:-152.0 eps:0.377\n",
            "Episode#12600 reward:-200.0 best_reward:-152.0 eps:0.372\n",
            "Episode#12700 reward:-200.0 best_reward:-152.0 eps:0.367\n",
            "Episode#12800 reward:-200.0 best_reward:-152.0 eps:0.362\n",
            "Episode#12900 reward:-200.0 best_reward:-149.0 eps:0.357\n",
            "Episode#13000 reward:-200.0 best_reward:-149.0 eps:0.352\n",
            "Episode#13100 reward:-200.0 best_reward:-149.0 eps:0.347\n",
            "Episode#13200 reward:-200.0 best_reward:-149.0 eps:0.342\n",
            "Episode#13300 reward:-190.0 best_reward:-149.0 eps:0.337\n",
            "Episode#13400 reward:-200.0 best_reward:-149.0 eps:0.333\n",
            "Episode#13500 reward:-200.0 best_reward:-149.0 eps:0.328\n",
            "Episode#13600 reward:-161.0 best_reward:-149.0 eps:0.323\n",
            "Episode#13700 reward:-200.0 best_reward:-149.0 eps:0.319\n",
            "Episode#13800 reward:-200.0 best_reward:-149.0 eps:0.314\n",
            "Episode#13900 reward:-163.0 best_reward:-149.0 eps:0.310\n",
            "Episode#14000 reward:-158.0 best_reward:-149.0 eps:0.305\n",
            "Episode#14100 reward:-200.0 best_reward:-149.0 eps:0.300\n",
            "Episode#14200 reward:-200.0 best_reward:-149.0 eps:0.295\n",
            "Episode#14300 reward:-200.0 best_reward:-149.0 eps:0.290\n",
            "Episode#14400 reward:-200.0 best_reward:-149.0 eps:0.285\n",
            "Episode#14500 reward:-200.0 best_reward:-139.0 eps:0.281\n",
            "Episode#14600 reward:-200.0 best_reward:-139.0 eps:0.276\n",
            "Episode#14700 reward:-157.0 best_reward:-139.0 eps:0.271\n",
            "Episode#14800 reward:-161.0 best_reward:-139.0 eps:0.266\n",
            "Episode#14900 reward:-200.0 best_reward:-139.0 eps:0.261\n",
            "Episode#15000 reward:-191.0 best_reward:-139.0 eps:0.257\n",
            "Episode#15100 reward:-200.0 best_reward:-139.0 eps:0.252\n",
            "Episode#15200 reward:-164.0 best_reward:-139.0 eps:0.248\n",
            "Episode#15300 reward:-200.0 best_reward:-139.0 eps:0.243\n",
            "Episode#15400 reward:-200.0 best_reward:-139.0 eps:0.239\n",
            "Episode#15500 reward:-200.0 best_reward:-139.0 eps:0.234\n",
            "Episode#15600 reward:-200.0 best_reward:-139.0 eps:0.229\n",
            "Episode#15700 reward:-200.0 best_reward:-139.0 eps:0.224\n",
            "Episode#15800 reward:-180.0 best_reward:-128.0 eps:0.219\n",
            "Episode#15900 reward:-172.0 best_reward:-128.0 eps:0.215\n",
            "Episode#16000 reward:-189.0 best_reward:-119.0 eps:0.210\n",
            "Episode#16100 reward:-162.0 best_reward:-119.0 eps:0.206\n",
            "Episode#16200 reward:-160.0 best_reward:-119.0 eps:0.202\n",
            "Episode#16300 reward:-187.0 best_reward:-119.0 eps:0.197\n",
            "Episode#16400 reward:-152.0 best_reward:-119.0 eps:0.193\n",
            "Episode#16500 reward:-156.0 best_reward:-119.0 eps:0.189\n",
            "Episode#16600 reward:-180.0 best_reward:-119.0 eps:0.185\n",
            "Episode#16700 reward:-153.0 best_reward:-119.0 eps:0.180\n",
            "Episode#16800 reward:-166.0 best_reward:-114.0 eps:0.176\n",
            "Episode#16900 reward:-125.0 best_reward:-114.0 eps:0.172\n",
            "Episode#17000 reward:-200.0 best_reward:-114.0 eps:0.168\n",
            "Episode#17100 reward:-190.0 best_reward:-114.0 eps:0.163\n",
            "Episode#17200 reward:-151.0 best_reward:-114.0 eps:0.159\n",
            "Episode#17300 reward:-158.0 best_reward:-114.0 eps:0.155\n",
            "Episode#17400 reward:-162.0 best_reward:-114.0 eps:0.151\n",
            "Episode#17500 reward:-151.0 best_reward:-114.0 eps:0.147\n",
            "Episode#17600 reward:-165.0 best_reward:-114.0 eps:0.142\n",
            "Episode#17700 reward:-118.0 best_reward:-114.0 eps:0.138\n",
            "Episode#17800 reward:-177.0 best_reward:-114.0 eps:0.134\n",
            "Episode#17900 reward:-200.0 best_reward:-114.0 eps:0.129\n",
            "Episode#18000 reward:-200.0 best_reward:-114.0 eps:0.124\n",
            "Episode#18100 reward:-197.0 best_reward:-114.0 eps:0.119\n",
            "Episode#18200 reward:-160.0 best_reward:-114.0 eps:0.115\n",
            "Episode#18300 reward:-155.0 best_reward:-114.0 eps:0.111\n",
            "Episode#18400 reward:-153.0 best_reward:-114.0 eps:0.107\n",
            "Episode#18500 reward:-160.0 best_reward:-114.0 eps:0.102\n",
            "Episode#18600 reward:-192.0 best_reward:-114.0 eps:0.098\n",
            "Episode#18700 reward:-158.0 best_reward:-114.0 eps:0.093\n",
            "Episode#18800 reward:-159.0 best_reward:-114.0 eps:0.089\n",
            "Episode#18900 reward:-163.0 best_reward:-114.0 eps:0.085\n",
            "Episode#19000 reward:-200.0 best_reward:-114.0 eps:0.080\n",
            "Episode#19100 reward:-200.0 best_reward:-114.0 eps:0.076\n",
            "Episode#19200 reward:-200.0 best_reward:-114.0 eps:0.071\n",
            "Episode#19300 reward:-200.0 best_reward:-114.0 eps:0.066\n",
            "Episode#19400 reward:-180.0 best_reward:-114.0 eps:0.062\n",
            "Episode#19500 reward:-158.0 best_reward:-114.0 eps:0.057\n",
            "Episode#19600 reward:-151.0 best_reward:-114.0 eps:0.053\n",
            "Episode#19700 reward:-200.0 best_reward:-114.0 eps:0.049\n",
            "Episode#19800 reward:-158.0 best_reward:-114.0 eps:0.045\n",
            "Episode#19900 reward:-200.0 best_reward:-114.0 eps:0.041\n",
            "Episode#20000 reward:-157.0 best_reward:-114.0 eps:0.036\n",
            "Episode#20100 reward:-161.0 best_reward:-114.0 eps:0.032\n",
            "Episode#20200 reward:-158.0 best_reward:-91.0 eps:0.028\n",
            "Episode#20300 reward:-188.0 best_reward:-91.0 eps:0.023\n",
            "Episode#20400 reward:-200.0 best_reward:-91.0 eps:0.019\n",
            "Episode#20500 reward:-158.0 best_reward:-91.0 eps:0.015\n",
            "Episode#20600 reward:-172.0 best_reward:-91.0 eps:0.011\n",
            "Episode#20700 reward:-147.0 best_reward:-91.0 eps:0.006\n",
            "Episode#20800 reward:-190.0 best_reward:-91.0 eps:0.005\n",
            "Episode#20900 reward:-149.0 best_reward:-91.0 eps:0.005\n",
            "Episode#21000 reward:-156.0 best_reward:-91.0 eps:0.005\n",
            "Episode#21100 reward:-181.0 best_reward:-91.0 eps:0.005\n",
            "Episode#21200 reward:-151.0 best_reward:-91.0 eps:0.005\n",
            "Episode#21300 reward:-199.0 best_reward:-91.0 eps:0.005\n",
            "Episode#21400 reward:-191.0 best_reward:-91.0 eps:0.005\n",
            "Episode#21500 reward:-154.0 best_reward:-91.0 eps:0.005\n",
            "Episode#21600 reward:-157.0 best_reward:-91.0 eps:0.005\n",
            "Episode#21700 reward:-195.0 best_reward:-91.0 eps:0.005\n",
            "Episode#21800 reward:-153.0 best_reward:-91.0 eps:0.005\n",
            "Episode#21900 reward:-159.0 best_reward:-91.0 eps:0.005\n",
            "Episode#22000 reward:-148.0 best_reward:-91.0 eps:0.005\n",
            "Episode#22100 reward:-169.0 best_reward:-91.0 eps:0.005\n",
            "Episode#22200 reward:-200.0 best_reward:-91.0 eps:0.005\n",
            "Episode#22300 reward:-149.0 best_reward:-91.0 eps:0.005\n",
            "Episode#22400 reward:-148.0 best_reward:-91.0 eps:0.005\n",
            "Episode#22500 reward:-115.0 best_reward:-91.0 eps:0.005\n",
            "Episode#22600 reward:-196.0 best_reward:-91.0 eps:0.005\n",
            "Episode#22700 reward:-200.0 best_reward:-91.0 eps:0.005\n",
            "Episode#22800 reward:-186.0 best_reward:-91.0 eps:0.005\n",
            "Episode#22900 reward:-163.0 best_reward:-91.0 eps:0.005\n",
            "Episode#23000 reward:-151.0 best_reward:-91.0 eps:0.005\n",
            "Episode#23100 reward:-200.0 best_reward:-91.0 eps:0.005\n",
            "Episode#23200 reward:-125.0 best_reward:-91.0 eps:0.005\n",
            "Episode#23300 reward:-200.0 best_reward:-91.0 eps:0.005\n",
            "Episode#23400 reward:-152.0 best_reward:-91.0 eps:0.005\n",
            "Episode#23500 reward:-189.0 best_reward:-91.0 eps:0.005\n",
            "Episode#23600 reward:-158.0 best_reward:-91.0 eps:0.005\n",
            "Episode#23700 reward:-150.0 best_reward:-91.0 eps:0.005\n",
            "Episode#23800 reward:-152.0 best_reward:-91.0 eps:0.005\n",
            "Episode#23900 reward:-177.0 best_reward:-91.0 eps:0.005\n",
            "Episode#24000 reward:-154.0 best_reward:-91.0 eps:0.005\n",
            "Episode#24100 reward:-154.0 best_reward:-91.0 eps:0.005\n",
            "Episode#24200 reward:-162.0 best_reward:-91.0 eps:0.005\n",
            "Episode#24300 reward:-147.0 best_reward:-91.0 eps:0.005\n",
            "Episode#24400 reward:-147.0 best_reward:-91.0 eps:0.005\n",
            "Episode#24500 reward:-150.0 best_reward:-91.0 eps:0.005\n",
            "Episode#24600 reward:-191.0 best_reward:-91.0 eps:0.005\n",
            "Episode#24700 reward:-159.0 best_reward:-91.0 eps:0.005\n",
            "Episode#24800 reward:-142.0 best_reward:-91.0 eps:0.005\n",
            "Episode#24900 reward:-200.0 best_reward:-91.0 eps:0.005\n",
            "Episode#25000 reward:-149.0 best_reward:-91.0 eps:0.005\n",
            "Episode#25100 reward:-158.0 best_reward:-91.0 eps:0.005\n",
            "Episode#25200 reward:-160.0 best_reward:-91.0 eps:0.005\n",
            "Episode#25300 reward:-153.0 best_reward:-91.0 eps:0.005\n",
            "Episode#25400 reward:-149.0 best_reward:-91.0 eps:0.005\n",
            "Episode#25500 reward:-152.0 best_reward:-91.0 eps:0.005\n",
            "Episode#25600 reward:-200.0 best_reward:-91.0 eps:0.005\n",
            "Episode#25700 reward:-148.0 best_reward:-91.0 eps:0.005\n",
            "Episode#25800 reward:-146.0 best_reward:-91.0 eps:0.005\n",
            "Episode#25900 reward:-200.0 best_reward:-91.0 eps:0.005\n",
            "Episode#26000 reward:-157.0 best_reward:-91.0 eps:0.005\n",
            "Episode#26100 reward:-148.0 best_reward:-91.0 eps:0.005\n",
            "Episode#26200 reward:-200.0 best_reward:-91.0 eps:0.005\n",
            "Episode#26300 reward:-136.0 best_reward:-91.0 eps:0.005\n",
            "Episode#26400 reward:-152.0 best_reward:-91.0 eps:0.005\n",
            "Episode#26500 reward:-145.0 best_reward:-91.0 eps:0.005\n",
            "Episode#26600 reward:-155.0 best_reward:-91.0 eps:0.005\n",
            "Episode#26700 reward:-158.0 best_reward:-91.0 eps:0.005\n",
            "Episode#26800 reward:-131.0 best_reward:-91.0 eps:0.005\n",
            "Episode#26900 reward:-152.0 best_reward:-91.0 eps:0.005\n",
            "Episode#27000 reward:-113.0 best_reward:-91.0 eps:0.005\n",
            "Episode#27100 reward:-200.0 best_reward:-88.0 eps:0.005\n",
            "Episode#27200 reward:-200.0 best_reward:-88.0 eps:0.005\n",
            "Episode#27300 reward:-139.0 best_reward:-88.0 eps:0.005\n",
            "Episode#27400 reward:-144.0 best_reward:-88.0 eps:0.005\n",
            "Episode#27500 reward:-159.0 best_reward:-88.0 eps:0.005\n",
            "Episode#27600 reward:-176.0 best_reward:-88.0 eps:0.005\n",
            "Episode#27700 reward:-125.0 best_reward:-88.0 eps:0.005\n",
            "Episode#27800 reward:-156.0 best_reward:-88.0 eps:0.005\n",
            "Episode#27900 reward:-150.0 best_reward:-88.0 eps:0.005\n",
            "Episode#28000 reward:-153.0 best_reward:-88.0 eps:0.005\n",
            "Episode#28100 reward:-194.0 best_reward:-88.0 eps:0.005\n",
            "Episode#28200 reward:-155.0 best_reward:-88.0 eps:0.005\n",
            "Episode#28300 reward:-184.0 best_reward:-88.0 eps:0.005\n",
            "Episode#28400 reward:-154.0 best_reward:-88.0 eps:0.005\n",
            "Episode#28500 reward:-147.0 best_reward:-88.0 eps:0.005\n",
            "Episode#28600 reward:-151.0 best_reward:-88.0 eps:0.005\n",
            "Episode#28700 reward:-149.0 best_reward:-88.0 eps:0.005\n",
            "Episode#28800 reward:-149.0 best_reward:-88.0 eps:0.005\n",
            "Episode#28900 reward:-150.0 best_reward:-88.0 eps:0.005\n",
            "Episode#29000 reward:-147.0 best_reward:-88.0 eps:0.005\n",
            "Episode#29100 reward:-150.0 best_reward:-88.0 eps:0.005\n",
            "Episode#29200 reward:-179.0 best_reward:-88.0 eps:0.005\n",
            "Episode#29300 reward:-152.0 best_reward:-88.0 eps:0.005\n",
            "Episode#29400 reward:-153.0 best_reward:-88.0 eps:0.005\n",
            "Episode#29500 reward:-114.0 best_reward:-88.0 eps:0.005\n",
            "Episode#29600 reward:-115.0 best_reward:-88.0 eps:0.005\n",
            "Episode#29700 reward:-146.0 best_reward:-88.0 eps:0.005\n",
            "Episode#29800 reward:-154.0 best_reward:-88.0 eps:0.005\n",
            "Episode#29900 reward:-143.0 best_reward:-88.0 eps:0.005\n",
            "Episode#30000 reward:-151.0 best_reward:-88.0 eps:0.005\n",
            "Episode#30100 reward:-145.0 best_reward:-88.0 eps:0.005\n",
            "Episode#30200 reward:-151.0 best_reward:-88.0 eps:0.005\n",
            "Episode#30300 reward:-148.0 best_reward:-88.0 eps:0.005\n",
            "Episode#30400 reward:-143.0 best_reward:-88.0 eps:0.005\n",
            "Episode#30500 reward:-141.0 best_reward:-88.0 eps:0.005\n",
            "Episode#30600 reward:-149.0 best_reward:-88.0 eps:0.005\n",
            "Episode#30700 reward:-150.0 best_reward:-88.0 eps:0.005\n",
            "Episode#30800 reward:-148.0 best_reward:-88.0 eps:0.005\n",
            "Episode#30900 reward:-150.0 best_reward:-88.0 eps:0.005\n",
            "Episode#31000 reward:-142.0 best_reward:-88.0 eps:0.005\n",
            "Episode#31100 reward:-143.0 best_reward:-88.0 eps:0.005\n",
            "Episode#31200 reward:-196.0 best_reward:-88.0 eps:0.005\n",
            "Episode#31300 reward:-192.0 best_reward:-88.0 eps:0.005\n",
            "Episode#31400 reward:-156.0 best_reward:-88.0 eps:0.005\n",
            "Episode#31500 reward:-153.0 best_reward:-88.0 eps:0.005\n",
            "Episode#31600 reward:-144.0 best_reward:-88.0 eps:0.005\n",
            "Episode#31700 reward:-157.0 best_reward:-88.0 eps:0.005\n",
            "Episode#31800 reward:-151.0 best_reward:-88.0 eps:0.005\n",
            "Episode#31900 reward:-156.0 best_reward:-88.0 eps:0.005\n",
            "Episode#32000 reward:-113.0 best_reward:-88.0 eps:0.005\n",
            "Episode#32100 reward:-153.0 best_reward:-88.0 eps:0.005\n",
            "Episode#32200 reward:-112.0 best_reward:-88.0 eps:0.005\n",
            "Episode#32300 reward:-146.0 best_reward:-88.0 eps:0.005\n",
            "Episode#32400 reward:-140.0 best_reward:-88.0 eps:0.005\n",
            "Episode#32500 reward:-144.0 best_reward:-88.0 eps:0.005\n",
            "Episode#32600 reward:-143.0 best_reward:-88.0 eps:0.005\n",
            "Episode#32700 reward:-139.0 best_reward:-88.0 eps:0.005\n",
            "Episode#32800 reward:-142.0 best_reward:-88.0 eps:0.005\n",
            "Episode#32900 reward:-136.0 best_reward:-88.0 eps:0.005\n",
            "Episode#33000 reward:-148.0 best_reward:-88.0 eps:0.005\n",
            "Episode#33100 reward:-112.0 best_reward:-88.0 eps:0.005\n",
            "Episode#33200 reward:-153.0 best_reward:-88.0 eps:0.005\n",
            "Episode#33300 reward:-113.0 best_reward:-88.0 eps:0.005\n",
            "Episode#33400 reward:-142.0 best_reward:-88.0 eps:0.005\n",
            "Episode#33500 reward:-111.0 best_reward:-88.0 eps:0.005\n",
            "Episode#33600 reward:-140.0 best_reward:-88.0 eps:0.005\n",
            "Episode#33700 reward:-158.0 best_reward:-88.0 eps:0.005\n",
            "Episode#33800 reward:-113.0 best_reward:-88.0 eps:0.005\n",
            "Episode#33900 reward:-141.0 best_reward:-88.0 eps:0.005\n",
            "Episode#34000 reward:-141.0 best_reward:-88.0 eps:0.005\n",
            "Episode#34100 reward:-117.0 best_reward:-88.0 eps:0.005\n",
            "Episode#34200 reward:-138.0 best_reward:-88.0 eps:0.005\n",
            "Episode#34300 reward:-146.0 best_reward:-88.0 eps:0.005\n",
            "Episode#34400 reward:-139.0 best_reward:-88.0 eps:0.005\n",
            "Episode#34500 reward:-122.0 best_reward:-88.0 eps:0.005\n",
            "Episode#34600 reward:-121.0 best_reward:-88.0 eps:0.005\n",
            "Episode#34700 reward:-141.0 best_reward:-88.0 eps:0.005\n",
            "Episode#34800 reward:-117.0 best_reward:-88.0 eps:0.005\n",
            "Episode#34900 reward:-116.0 best_reward:-88.0 eps:0.005\n",
            "Episode#35000 reward:-116.0 best_reward:-88.0 eps:0.005\n",
            "Episode#35100 reward:-114.0 best_reward:-88.0 eps:0.005\n",
            "Episode#35200 reward:-143.0 best_reward:-88.0 eps:0.005\n",
            "Episode#35300 reward:-135.0 best_reward:-88.0 eps:0.005\n",
            "Episode#35400 reward:-140.0 best_reward:-88.0 eps:0.005\n",
            "Episode#35500 reward:-127.0 best_reward:-88.0 eps:0.005\n",
            "Episode#35600 reward:-150.0 best_reward:-88.0 eps:0.005\n",
            "Episode#35700 reward:-198.0 best_reward:-88.0 eps:0.005\n",
            "Episode#35800 reward:-157.0 best_reward:-88.0 eps:0.005\n",
            "Episode#35900 reward:-115.0 best_reward:-88.0 eps:0.005\n",
            "Episode#36000 reward:-125.0 best_reward:-88.0 eps:0.005\n",
            "Episode#36100 reward:-188.0 best_reward:-88.0 eps:0.005\n",
            "Episode#36200 reward:-200.0 best_reward:-88.0 eps:0.005\n",
            "Episode#36300 reward:-121.0 best_reward:-88.0 eps:0.005\n",
            "Episode#36400 reward:-114.0 best_reward:-88.0 eps:0.005\n",
            "Episode#36500 reward:-121.0 best_reward:-88.0 eps:0.005\n",
            "Episode#36600 reward:-128.0 best_reward:-88.0 eps:0.005\n",
            "Episode#36700 reward:-125.0 best_reward:-88.0 eps:0.005\n",
            "Episode#36800 reward:-191.0 best_reward:-88.0 eps:0.005\n",
            "Episode#36900 reward:-200.0 best_reward:-88.0 eps:0.005\n",
            "Episode#37000 reward:-189.0 best_reward:-88.0 eps:0.005\n",
            "Episode#37100 reward:-188.0 best_reward:-88.0 eps:0.005\n",
            "Episode#37200 reward:-194.0 best_reward:-88.0 eps:0.005\n",
            "Episode#37300 reward:-166.0 best_reward:-88.0 eps:0.005\n",
            "Episode#37400 reward:-200.0 best_reward:-88.0 eps:0.005\n",
            "Episode#37500 reward:-147.0 best_reward:-88.0 eps:0.005\n",
            "Episode#37600 reward:-189.0 best_reward:-88.0 eps:0.005\n",
            "Episode#37700 reward:-150.0 best_reward:-88.0 eps:0.005\n",
            "Episode#37800 reward:-184.0 best_reward:-88.0 eps:0.005\n",
            "Episode#37900 reward:-176.0 best_reward:-88.0 eps:0.005\n",
            "Episode#38000 reward:-165.0 best_reward:-88.0 eps:0.005\n",
            "Episode#38100 reward:-194.0 best_reward:-88.0 eps:0.005\n",
            "Episode#38200 reward:-195.0 best_reward:-88.0 eps:0.005\n",
            "Episode#38300 reward:-148.0 best_reward:-88.0 eps:0.005\n",
            "Episode#38400 reward:-142.0 best_reward:-88.0 eps:0.005\n",
            "Episode#38500 reward:-121.0 best_reward:-88.0 eps:0.005\n",
            "Episode#38600 reward:-161.0 best_reward:-88.0 eps:0.005\n",
            "Episode#38700 reward:-170.0 best_reward:-88.0 eps:0.005\n",
            "Episode#38800 reward:-126.0 best_reward:-88.0 eps:0.005\n",
            "Episode#38900 reward:-131.0 best_reward:-88.0 eps:0.005\n",
            "Episode#39000 reward:-160.0 best_reward:-88.0 eps:0.005\n",
            "Episode#39100 reward:-154.0 best_reward:-88.0 eps:0.005\n",
            "Episode#39200 reward:-200.0 best_reward:-88.0 eps:0.005\n",
            "Episode#39300 reward:-129.0 best_reward:-88.0 eps:0.005\n",
            "Episode#39400 reward:-146.0 best_reward:-88.0 eps:0.005\n",
            "Episode#39500 reward:-153.0 best_reward:-88.0 eps:0.005\n",
            "Episode#39600 reward:-121.0 best_reward:-88.0 eps:0.005\n",
            "Episode#39700 reward:-140.0 best_reward:-88.0 eps:0.005\n",
            "Episode#39800 reward:-147.0 best_reward:-88.0 eps:0.005\n",
            "Episode#39900 reward:-146.0 best_reward:-88.0 eps:0.005\n",
            "Episode#40000 reward:-145.0 best_reward:-88.0 eps:0.005\n",
            "Episode#40100 reward:-139.0 best_reward:-88.0 eps:0.005\n",
            "Episode#40200 reward:-143.0 best_reward:-88.0 eps:0.005\n",
            "Episode#40300 reward:-116.0 best_reward:-88.0 eps:0.005\n",
            "Episode#40400 reward:-145.0 best_reward:-88.0 eps:0.005\n",
            "Episode#40500 reward:-148.0 best_reward:-88.0 eps:0.005\n",
            "Episode#40600 reward:-116.0 best_reward:-88.0 eps:0.005\n",
            "Episode#40700 reward:-145.0 best_reward:-88.0 eps:0.005\n",
            "Episode#40800 reward:-146.0 best_reward:-88.0 eps:0.005\n",
            "Episode#40900 reward:-153.0 best_reward:-88.0 eps:0.005\n",
            "Episode#41000 reward:-137.0 best_reward:-88.0 eps:0.005\n",
            "Episode#41100 reward:-113.0 best_reward:-88.0 eps:0.005\n",
            "Episode#41200 reward:-158.0 best_reward:-88.0 eps:0.005\n",
            "Episode#41300 reward:-145.0 best_reward:-88.0 eps:0.005\n",
            "Episode#41400 reward:-152.0 best_reward:-88.0 eps:0.005\n",
            "Episode#41500 reward:-200.0 best_reward:-88.0 eps:0.005\n",
            "Episode#41600 reward:-112.0 best_reward:-88.0 eps:0.005\n",
            "Episode#41700 reward:-141.0 best_reward:-88.0 eps:0.005\n",
            "Episode#41800 reward:-153.0 best_reward:-88.0 eps:0.005\n",
            "Episode#41900 reward:-145.0 best_reward:-88.0 eps:0.005\n",
            "Episode#42000 reward:-200.0 best_reward:-88.0 eps:0.005\n",
            "Episode#42100 reward:-134.0 best_reward:-88.0 eps:0.005\n",
            "Episode#42200 reward:-115.0 best_reward:-88.0 eps:0.005\n",
            "Episode#42300 reward:-153.0 best_reward:-88.0 eps:0.005\n",
            "Episode#42400 reward:-149.0 best_reward:-88.0 eps:0.005\n",
            "Episode#42500 reward:-114.0 best_reward:-88.0 eps:0.005\n",
            "Episode#42600 reward:-113.0 best_reward:-88.0 eps:0.005\n",
            "Episode#42700 reward:-158.0 best_reward:-88.0 eps:0.005\n",
            "Episode#42800 reward:-155.0 best_reward:-88.0 eps:0.005\n",
            "Episode#42900 reward:-143.0 best_reward:-88.0 eps:0.005\n",
            "Episode#43000 reward:-186.0 best_reward:-88.0 eps:0.005\n",
            "Episode#43100 reward:-155.0 best_reward:-88.0 eps:0.005\n",
            "Episode#43200 reward:-120.0 best_reward:-88.0 eps:0.005\n",
            "Episode#43300 reward:-157.0 best_reward:-88.0 eps:0.005\n",
            "Episode#43400 reward:-118.0 best_reward:-88.0 eps:0.005\n",
            "Episode#43500 reward:-140.0 best_reward:-88.0 eps:0.005\n",
            "Episode#43600 reward:-123.0 best_reward:-88.0 eps:0.005\n",
            "Episode#43700 reward:-142.0 best_reward:-88.0 eps:0.005\n",
            "Episode#43800 reward:-157.0 best_reward:-88.0 eps:0.005\n",
            "Episode#43900 reward:-126.0 best_reward:-88.0 eps:0.005\n",
            "Episode#44000 reward:-150.0 best_reward:-88.0 eps:0.005\n",
            "Episode#44100 reward:-155.0 best_reward:-88.0 eps:0.005\n",
            "Episode#44200 reward:-149.0 best_reward:-88.0 eps:0.005\n",
            "Episode#44300 reward:-120.0 best_reward:-88.0 eps:0.005\n",
            "Episode#44400 reward:-200.0 best_reward:-88.0 eps:0.005\n",
            "Episode#44500 reward:-190.0 best_reward:-88.0 eps:0.005\n",
            "Episode#44600 reward:-143.0 best_reward:-88.0 eps:0.005\n",
            "Episode#44700 reward:-157.0 best_reward:-88.0 eps:0.005\n",
            "Episode#44800 reward:-126.0 best_reward:-88.0 eps:0.005\n",
            "Episode#44900 reward:-146.0 best_reward:-88.0 eps:0.005\n",
            "Episode#45000 reward:-175.0 best_reward:-88.0 eps:0.005\n",
            "Episode#45100 reward:-200.0 best_reward:-88.0 eps:0.005\n",
            "Episode#45200 reward:-120.0 best_reward:-88.0 eps:0.005\n",
            "Episode#45300 reward:-122.0 best_reward:-88.0 eps:0.005\n",
            "Episode#45400 reward:-95.0 best_reward:-88.0 eps:0.005\n",
            "Episode#45500 reward:-94.0 best_reward:-88.0 eps:0.005\n",
            "Episode#45600 reward:-186.0 best_reward:-88.0 eps:0.005\n",
            "Episode#45700 reward:-120.0 best_reward:-88.0 eps:0.005\n",
            "Episode#45800 reward:-119.0 best_reward:-88.0 eps:0.005\n",
            "Episode#45900 reward:-112.0 best_reward:-88.0 eps:0.005\n",
            "Episode#46000 reward:-111.0 best_reward:-88.0 eps:0.005\n",
            "Episode#46100 reward:-109.0 best_reward:-88.0 eps:0.005\n",
            "Episode#46200 reward:-114.0 best_reward:-88.0 eps:0.005\n",
            "Episode#46300 reward:-115.0 best_reward:-88.0 eps:0.005\n",
            "Episode#46400 reward:-110.0 best_reward:-88.0 eps:0.005\n",
            "Episode#46500 reward:-114.0 best_reward:-88.0 eps:0.005\n",
            "Episode#46600 reward:-130.0 best_reward:-88.0 eps:0.005\n",
            "Episode#46700 reward:-154.0 best_reward:-88.0 eps:0.005\n",
            "Episode#46800 reward:-150.0 best_reward:-88.0 eps:0.005\n",
            "Episode#46900 reward:-183.0 best_reward:-88.0 eps:0.005\n",
            "Episode#47000 reward:-158.0 best_reward:-88.0 eps:0.005\n",
            "Episode#47100 reward:-189.0 best_reward:-88.0 eps:0.005\n",
            "Episode#47200 reward:-142.0 best_reward:-88.0 eps:0.005\n",
            "Episode#47300 reward:-169.0 best_reward:-88.0 eps:0.005\n",
            "Episode#47400 reward:-194.0 best_reward:-88.0 eps:0.005\n",
            "Episode#47500 reward:-152.0 best_reward:-88.0 eps:0.005\n",
            "Episode#47600 reward:-153.0 best_reward:-88.0 eps:0.005\n",
            "Episode#47700 reward:-155.0 best_reward:-88.0 eps:0.005\n",
            "Episode#47800 reward:-179.0 best_reward:-88.0 eps:0.005\n",
            "Episode#47900 reward:-183.0 best_reward:-88.0 eps:0.005\n",
            "Episode#48000 reward:-167.0 best_reward:-88.0 eps:0.005\n",
            "Episode#48100 reward:-155.0 best_reward:-88.0 eps:0.005\n",
            "Episode#48200 reward:-200.0 best_reward:-88.0 eps:0.005\n",
            "Episode#48300 reward:-152.0 best_reward:-88.0 eps:0.005\n",
            "Episode#48400 reward:-143.0 best_reward:-88.0 eps:0.005\n",
            "Episode#48500 reward:-184.0 best_reward:-88.0 eps:0.005\n",
            "Episode#48600 reward:-144.0 best_reward:-88.0 eps:0.005\n",
            "Episode#48700 reward:-190.0 best_reward:-88.0 eps:0.005\n",
            "Episode#48800 reward:-167.0 best_reward:-88.0 eps:0.005\n",
            "Episode#48900 reward:-140.0 best_reward:-88.0 eps:0.005\n",
            "Episode#49000 reward:-152.0 best_reward:-88.0 eps:0.005\n",
            "Episode#49100 reward:-188.0 best_reward:-88.0 eps:0.005\n",
            "Episode#49200 reward:-143.0 best_reward:-88.0 eps:0.005\n",
            "Episode#49300 reward:-144.0 best_reward:-88.0 eps:0.005\n",
            "Episode#49400 reward:-133.0 best_reward:-88.0 eps:0.005\n",
            "Episode#49500 reward:-142.0 best_reward:-88.0 eps:0.005\n",
            "Episode#49600 reward:-152.0 best_reward:-88.0 eps:0.005\n",
            "Episode#49700 reward:-118.0 best_reward:-88.0 eps:0.005\n",
            "Episode#49800 reward:-141.0 best_reward:-88.0 eps:0.005\n",
            "Episode#49900 reward:-142.0 best_reward:-88.0 eps:0.005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video /content/videos/rl-video-episode-0.mp4.\n",
            "Moviepy - Writing video /content/videos/rl-video-episode-0.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/rl-video-episode-0.mp4\n",
            "Test episode reward: -141.0\n",
            "Moviepy - Building video /content/videos/rl-video-episode-1.mp4.\n",
            "Moviepy - Writing video /content/videos/rl-video-episode-1.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/rl-video-episode-1.mp4\n",
            "Test episode reward: -140.0\n",
            "Test episode reward: -142.0\n",
            "Test episode reward: -150.0\n",
            "Test episode reward: -124.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SARSA Learner"
      ],
      "metadata": {
        "id": "moyj4_shqmXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "MAX_NUM_EPISODES = 50000\n",
        "STEPS_PER_EPISODE = 200\n",
        "EPSILON_MIN = 0.005\n",
        "max_num_steps = MAX_NUM_EPISODES * STEPS_PER_EPISODE\n",
        "EPSILON_DECAY = 500 * EPSILON_MIN / max_num_steps\n",
        "ALPHA = 0.05\n",
        "GAMMA = 0.98\n",
        "NUM_DISCRETE_BINS = 30\n",
        "\n",
        "class SARSA_Learner(object):\n",
        "    def __init__(self, env):\n",
        "        self.obs_shape = env.observation_space.shape\n",
        "        self.obs_high = env.observation_space.high\n",
        "        self.obs_low = env.observation_space.low\n",
        "        self.obs_bins = NUM_DISCRETE_BINS\n",
        "        self.bin_width = (self.obs_high - self.obs_low) / self.obs_bins\n",
        "        self.action_shape = env.action_space.n\n",
        "        self.Q = np.zeros((self.obs_bins + 1, self.obs_bins + 1, self.action_shape))\n",
        "        self.alpha = ALPHA\n",
        "        self.gamma = GAMMA\n",
        "        self.epsilon = 1.0\n",
        "\n",
        "    def discretize(self, obs):\n",
        "        return tuple(((obs - self.obs_low) / self.bin_width).astype(int))\n",
        "\n",
        "    def get_action(self, obs):\n",
        "        discretized_obs = self.discretize(obs)\n",
        "        if self.epsilon > EPSILON_MIN:\n",
        "            self.epsilon -= EPSILON_DECAY\n",
        "        if np.random.random() > self.epsilon:\n",
        "            return np.argmax(self.Q[discretized_obs])\n",
        "        else:\n",
        "            return np.random.choice([a for a in range(self.action_shape)])\n",
        "\n",
        "    # Modified learn method for SARSA\n",
        "    def learn(self, obs, action, reward, next_obs, next_action):\n",
        "        discretized_obs = self.discretize(obs)\n",
        "        discretized_next_obs = self.discretize(next_obs)\n",
        "        # SARSA Update to use Q-value of next state-action pair instead of max Q-value\n",
        "        td_target = reward + self.gamma * self.Q[discretized_next_obs][next_action]\n",
        "        td_error = td_target - self.Q[discretized_obs][action]\n",
        "        self.Q[discretized_obs][action] += self.alpha * td_error\n",
        "\n",
        "def train(agent, env):\n",
        "    best_reward = -float('inf')\n",
        "    for episode in range(MAX_NUM_EPISODES):\n",
        "        obs, _ = env.reset()\n",
        "        total_reward = 0.0\n",
        "        # Choose initial action\n",
        "        action = agent.get_action(obs)  # Added this line for SARSA\n",
        "        for _ in range(STEPS_PER_EPISODE):\n",
        "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            next_action = agent.get_action(next_obs)  # Choose next action\n",
        "            # SARSA learning: pass current state-action and next state-action\n",
        "            agent.learn(obs, action, reward, next_obs, next_action)\n",
        "            obs = next_obs\n",
        "            action = next_action  # Update action for next iteration\n",
        "            total_reward += reward\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "        if total_reward > best_reward:\n",
        "            best_reward = total_reward\n",
        "        if episode % 100 == 0:\n",
        "            print(f\"Episode#{episode} reward:{total_reward} best_reward:{best_reward} eps:{agent.epsilon:.3f}\")\n",
        "    return np.argmax(agent.Q, axis=2)\n",
        "\n",
        "def test(agent, env, policy):\n",
        "    obs, _ = env.reset()\n",
        "    total_reward = 0.0\n",
        "    for _ in range(STEPS_PER_EPISODE):\n",
        "        action = policy[agent.discretize(obs)]\n",
        "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "        obs = next_obs\n",
        "        total_reward += reward\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "    return total_reward\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = gym.make('MountainCar-v0')\n",
        "    agent = SARSA_Learner(env)  # Changed to SARSA_Learner\n",
        "    learned_policy = train(agent, env)\n",
        "\n",
        "    test_env = gym.make('MountainCar-v0', render_mode='rgb_array')\n",
        "    test_env = gym.wrappers.RecordVideo(test_env, \"videos\")\n",
        "    for _ in range(20):  # Test for 5 episodes\n",
        "        total_reward = test(agent, test_env, learned_policy)\n",
        "        print(f\"Test episode reward: {total_reward}\")\n",
        "    test_env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4vvZ_55C0hu",
        "outputId": "9cf39d06-cf9b-43fc-e08b-fb543022d824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode#0 reward:-200.0 best_reward:-200.0 eps:1.000\n",
            "Episode#100 reward:-200.0 best_reward:-200.0 eps:0.995\n",
            "Episode#200 reward:-200.0 best_reward:-200.0 eps:0.990\n",
            "Episode#300 reward:-200.0 best_reward:-200.0 eps:0.985\n",
            "Episode#400 reward:-200.0 best_reward:-200.0 eps:0.980\n",
            "Episode#500 reward:-200.0 best_reward:-200.0 eps:0.975\n",
            "Episode#600 reward:-200.0 best_reward:-200.0 eps:0.970\n",
            "Episode#700 reward:-200.0 best_reward:-200.0 eps:0.965\n",
            "Episode#800 reward:-200.0 best_reward:-200.0 eps:0.960\n",
            "Episode#900 reward:-200.0 best_reward:-200.0 eps:0.955\n",
            "Episode#1000 reward:-200.0 best_reward:-200.0 eps:0.950\n",
            "Episode#1100 reward:-200.0 best_reward:-200.0 eps:0.945\n",
            "Episode#1200 reward:-200.0 best_reward:-200.0 eps:0.940\n",
            "Episode#1300 reward:-200.0 best_reward:-200.0 eps:0.935\n",
            "Episode#1400 reward:-200.0 best_reward:-200.0 eps:0.930\n",
            "Episode#1500 reward:-200.0 best_reward:-200.0 eps:0.925\n",
            "Episode#1600 reward:-200.0 best_reward:-200.0 eps:0.920\n",
            "Episode#1700 reward:-200.0 best_reward:-200.0 eps:0.915\n",
            "Episode#1800 reward:-200.0 best_reward:-200.0 eps:0.909\n",
            "Episode#1900 reward:-200.0 best_reward:-200.0 eps:0.904\n",
            "Episode#2000 reward:-200.0 best_reward:-200.0 eps:0.899\n",
            "Episode#2100 reward:-200.0 best_reward:-200.0 eps:0.894\n",
            "Episode#2200 reward:-200.0 best_reward:-200.0 eps:0.889\n",
            "Episode#2300 reward:-200.0 best_reward:-200.0 eps:0.884\n",
            "Episode#2400 reward:-200.0 best_reward:-200.0 eps:0.879\n",
            "Episode#2500 reward:-200.0 best_reward:-200.0 eps:0.874\n",
            "Episode#2600 reward:-200.0 best_reward:-200.0 eps:0.869\n",
            "Episode#2700 reward:-200.0 best_reward:-200.0 eps:0.864\n",
            "Episode#2800 reward:-200.0 best_reward:-200.0 eps:0.859\n",
            "Episode#2900 reward:-200.0 best_reward:-200.0 eps:0.854\n",
            "Episode#3000 reward:-200.0 best_reward:-200.0 eps:0.849\n",
            "Episode#3100 reward:-200.0 best_reward:-200.0 eps:0.844\n",
            "Episode#3200 reward:-200.0 best_reward:-200.0 eps:0.839\n",
            "Episode#3300 reward:-200.0 best_reward:-200.0 eps:0.834\n",
            "Episode#3400 reward:-200.0 best_reward:-200.0 eps:0.829\n",
            "Episode#3500 reward:-200.0 best_reward:-200.0 eps:0.824\n",
            "Episode#3600 reward:-200.0 best_reward:-200.0 eps:0.819\n",
            "Episode#3700 reward:-200.0 best_reward:-200.0 eps:0.814\n",
            "Episode#3800 reward:-200.0 best_reward:-200.0 eps:0.809\n",
            "Episode#3900 reward:-200.0 best_reward:-200.0 eps:0.804\n",
            "Episode#4000 reward:-200.0 best_reward:-200.0 eps:0.799\n",
            "Episode#4100 reward:-200.0 best_reward:-200.0 eps:0.794\n",
            "Episode#4200 reward:-200.0 best_reward:-200.0 eps:0.789\n",
            "Episode#4300 reward:-200.0 best_reward:-200.0 eps:0.784\n",
            "Episode#4400 reward:-200.0 best_reward:-200.0 eps:0.779\n",
            "Episode#4500 reward:-200.0 best_reward:-200.0 eps:0.774\n",
            "Episode#4600 reward:-200.0 best_reward:-200.0 eps:0.769\n",
            "Episode#4700 reward:-200.0 best_reward:-200.0 eps:0.764\n",
            "Episode#4800 reward:-200.0 best_reward:-200.0 eps:0.759\n",
            "Episode#4900 reward:-200.0 best_reward:-200.0 eps:0.754\n",
            "Episode#5000 reward:-200.0 best_reward:-200.0 eps:0.749\n",
            "Episode#5100 reward:-200.0 best_reward:-200.0 eps:0.744\n",
            "Episode#5200 reward:-200.0 best_reward:-200.0 eps:0.739\n",
            "Episode#5300 reward:-200.0 best_reward:-200.0 eps:0.734\n",
            "Episode#5400 reward:-200.0 best_reward:-200.0 eps:0.729\n",
            "Episode#5500 reward:-200.0 best_reward:-200.0 eps:0.724\n",
            "Episode#5600 reward:-200.0 best_reward:-200.0 eps:0.719\n",
            "Episode#5700 reward:-200.0 best_reward:-200.0 eps:0.714\n",
            "Episode#5800 reward:-200.0 best_reward:-200.0 eps:0.708\n",
            "Episode#5900 reward:-200.0 best_reward:-200.0 eps:0.703\n",
            "Episode#6000 reward:-200.0 best_reward:-200.0 eps:0.698\n",
            "Episode#6100 reward:-200.0 best_reward:-200.0 eps:0.693\n",
            "Episode#6200 reward:-200.0 best_reward:-200.0 eps:0.688\n",
            "Episode#6300 reward:-200.0 best_reward:-200.0 eps:0.683\n",
            "Episode#6400 reward:-200.0 best_reward:-200.0 eps:0.678\n",
            "Episode#6500 reward:-200.0 best_reward:-200.0 eps:0.673\n",
            "Episode#6600 reward:-200.0 best_reward:-200.0 eps:0.668\n",
            "Episode#6700 reward:-200.0 best_reward:-200.0 eps:0.663\n",
            "Episode#6800 reward:-200.0 best_reward:-200.0 eps:0.658\n",
            "Episode#6900 reward:-200.0 best_reward:-200.0 eps:0.653\n",
            "Episode#7000 reward:-200.0 best_reward:-200.0 eps:0.648\n",
            "Episode#7100 reward:-200.0 best_reward:-200.0 eps:0.643\n",
            "Episode#7200 reward:-200.0 best_reward:-200.0 eps:0.638\n",
            "Episode#7300 reward:-200.0 best_reward:-200.0 eps:0.633\n",
            "Episode#7400 reward:-200.0 best_reward:-200.0 eps:0.628\n",
            "Episode#7500 reward:-200.0 best_reward:-200.0 eps:0.623\n",
            "Episode#7600 reward:-200.0 best_reward:-200.0 eps:0.618\n",
            "Episode#7700 reward:-200.0 best_reward:-200.0 eps:0.613\n",
            "Episode#7800 reward:-200.0 best_reward:-200.0 eps:0.608\n",
            "Episode#7900 reward:-200.0 best_reward:-200.0 eps:0.603\n",
            "Episode#8000 reward:-200.0 best_reward:-200.0 eps:0.598\n",
            "Episode#8100 reward:-200.0 best_reward:-200.0 eps:0.593\n",
            "Episode#8200 reward:-200.0 best_reward:-200.0 eps:0.588\n",
            "Episode#8300 reward:-200.0 best_reward:-200.0 eps:0.583\n",
            "Episode#8400 reward:-200.0 best_reward:-169.0 eps:0.578\n",
            "Episode#8500 reward:-200.0 best_reward:-169.0 eps:0.573\n",
            "Episode#8600 reward:-200.0 best_reward:-169.0 eps:0.568\n",
            "Episode#8700 reward:-200.0 best_reward:-168.0 eps:0.563\n",
            "Episode#8800 reward:-200.0 best_reward:-168.0 eps:0.558\n",
            "Episode#8900 reward:-200.0 best_reward:-168.0 eps:0.553\n",
            "Episode#9000 reward:-200.0 best_reward:-168.0 eps:0.548\n",
            "Episode#9100 reward:-200.0 best_reward:-168.0 eps:0.543\n",
            "Episode#9200 reward:-200.0 best_reward:-168.0 eps:0.538\n",
            "Episode#9300 reward:-200.0 best_reward:-168.0 eps:0.533\n",
            "Episode#9400 reward:-200.0 best_reward:-168.0 eps:0.528\n",
            "Episode#9500 reward:-200.0 best_reward:-168.0 eps:0.523\n",
            "Episode#9600 reward:-200.0 best_reward:-162.0 eps:0.518\n",
            "Episode#9700 reward:-200.0 best_reward:-162.0 eps:0.513\n",
            "Episode#9800 reward:-200.0 best_reward:-162.0 eps:0.508\n",
            "Episode#9900 reward:-200.0 best_reward:-162.0 eps:0.503\n",
            "Episode#10000 reward:-200.0 best_reward:-162.0 eps:0.498\n",
            "Episode#10100 reward:-200.0 best_reward:-162.0 eps:0.493\n",
            "Episode#10200 reward:-200.0 best_reward:-162.0 eps:0.488\n",
            "Episode#10300 reward:-200.0 best_reward:-158.0 eps:0.483\n",
            "Episode#10400 reward:-200.0 best_reward:-158.0 eps:0.478\n",
            "Episode#10500 reward:-200.0 best_reward:-158.0 eps:0.473\n",
            "Episode#10600 reward:-200.0 best_reward:-158.0 eps:0.468\n",
            "Episode#10700 reward:-200.0 best_reward:-158.0 eps:0.463\n",
            "Episode#10800 reward:-200.0 best_reward:-155.0 eps:0.458\n",
            "Episode#10900 reward:-200.0 best_reward:-155.0 eps:0.453\n",
            "Episode#11000 reward:-167.0 best_reward:-153.0 eps:0.448\n",
            "Episode#11100 reward:-200.0 best_reward:-153.0 eps:0.443\n",
            "Episode#11200 reward:-200.0 best_reward:-153.0 eps:0.438\n",
            "Episode#11300 reward:-196.0 best_reward:-153.0 eps:0.433\n",
            "Episode#11400 reward:-166.0 best_reward:-152.0 eps:0.428\n",
            "Episode#11500 reward:-200.0 best_reward:-152.0 eps:0.424\n",
            "Episode#11600 reward:-167.0 best_reward:-151.0 eps:0.419\n",
            "Episode#11700 reward:-195.0 best_reward:-151.0 eps:0.414\n",
            "Episode#11800 reward:-200.0 best_reward:-151.0 eps:0.409\n",
            "Episode#11900 reward:-200.0 best_reward:-151.0 eps:0.404\n",
            "Episode#12000 reward:-174.0 best_reward:-151.0 eps:0.400\n",
            "Episode#12100 reward:-194.0 best_reward:-151.0 eps:0.395\n",
            "Episode#12200 reward:-200.0 best_reward:-151.0 eps:0.390\n",
            "Episode#12300 reward:-200.0 best_reward:-151.0 eps:0.385\n",
            "Episode#12400 reward:-159.0 best_reward:-151.0 eps:0.380\n",
            "Episode#12500 reward:-200.0 best_reward:-151.0 eps:0.375\n",
            "Episode#12600 reward:-200.0 best_reward:-151.0 eps:0.371\n",
            "Episode#12700 reward:-200.0 best_reward:-151.0 eps:0.366\n",
            "Episode#12800 reward:-200.0 best_reward:-120.0 eps:0.361\n",
            "Episode#12900 reward:-154.0 best_reward:-120.0 eps:0.356\n",
            "Episode#13000 reward:-200.0 best_reward:-120.0 eps:0.352\n",
            "Episode#13100 reward:-200.0 best_reward:-120.0 eps:0.347\n",
            "Episode#13200 reward:-164.0 best_reward:-120.0 eps:0.343\n",
            "Episode#13300 reward:-200.0 best_reward:-120.0 eps:0.338\n",
            "Episode#13400 reward:-188.0 best_reward:-120.0 eps:0.334\n",
            "Episode#13500 reward:-200.0 best_reward:-120.0 eps:0.329\n",
            "Episode#13600 reward:-200.0 best_reward:-120.0 eps:0.324\n",
            "Episode#13700 reward:-200.0 best_reward:-120.0 eps:0.320\n",
            "Episode#13800 reward:-200.0 best_reward:-120.0 eps:0.315\n",
            "Episode#13900 reward:-163.0 best_reward:-120.0 eps:0.311\n",
            "Episode#14000 reward:-171.0 best_reward:-120.0 eps:0.306\n",
            "Episode#14100 reward:-200.0 best_reward:-120.0 eps:0.301\n",
            "Episode#14200 reward:-200.0 best_reward:-120.0 eps:0.297\n",
            "Episode#14300 reward:-200.0 best_reward:-120.0 eps:0.292\n",
            "Episode#14400 reward:-200.0 best_reward:-120.0 eps:0.288\n",
            "Episode#14500 reward:-159.0 best_reward:-120.0 eps:0.283\n",
            "Episode#14600 reward:-200.0 best_reward:-120.0 eps:0.279\n",
            "Episode#14700 reward:-154.0 best_reward:-120.0 eps:0.274\n",
            "Episode#14800 reward:-159.0 best_reward:-120.0 eps:0.270\n",
            "Episode#14900 reward:-200.0 best_reward:-120.0 eps:0.265\n",
            "Episode#15000 reward:-164.0 best_reward:-120.0 eps:0.261\n",
            "Episode#15100 reward:-199.0 best_reward:-120.0 eps:0.256\n",
            "Episode#15200 reward:-159.0 best_reward:-120.0 eps:0.252\n",
            "Episode#15300 reward:-156.0 best_reward:-120.0 eps:0.248\n",
            "Episode#15400 reward:-196.0 best_reward:-120.0 eps:0.243\n",
            "Episode#15500 reward:-190.0 best_reward:-120.0 eps:0.239\n",
            "Episode#15600 reward:-152.0 best_reward:-117.0 eps:0.235\n",
            "Episode#15700 reward:-199.0 best_reward:-117.0 eps:0.230\n",
            "Episode#15800 reward:-154.0 best_reward:-117.0 eps:0.226\n",
            "Episode#15900 reward:-168.0 best_reward:-115.0 eps:0.222\n",
            "Episode#16000 reward:-150.0 best_reward:-115.0 eps:0.217\n",
            "Episode#16100 reward:-200.0 best_reward:-115.0 eps:0.213\n",
            "Episode#16200 reward:-155.0 best_reward:-103.0 eps:0.208\n",
            "Episode#16300 reward:-127.0 best_reward:-103.0 eps:0.204\n",
            "Episode#16400 reward:-160.0 best_reward:-103.0 eps:0.200\n",
            "Episode#16500 reward:-200.0 best_reward:-103.0 eps:0.195\n",
            "Episode#16600 reward:-160.0 best_reward:-103.0 eps:0.191\n",
            "Episode#16700 reward:-163.0 best_reward:-103.0 eps:0.187\n",
            "Episode#16800 reward:-200.0 best_reward:-103.0 eps:0.183\n",
            "Episode#16900 reward:-118.0 best_reward:-103.0 eps:0.178\n",
            "Episode#17000 reward:-160.0 best_reward:-103.0 eps:0.174\n",
            "Episode#17100 reward:-165.0 best_reward:-103.0 eps:0.170\n",
            "Episode#17200 reward:-154.0 best_reward:-103.0 eps:0.166\n",
            "Episode#17300 reward:-157.0 best_reward:-103.0 eps:0.162\n",
            "Episode#17400 reward:-153.0 best_reward:-103.0 eps:0.158\n",
            "Episode#17500 reward:-148.0 best_reward:-103.0 eps:0.154\n",
            "Episode#17600 reward:-169.0 best_reward:-103.0 eps:0.150\n",
            "Episode#17700 reward:-157.0 best_reward:-103.0 eps:0.146\n",
            "Episode#17800 reward:-200.0 best_reward:-103.0 eps:0.141\n",
            "Episode#17900 reward:-200.0 best_reward:-103.0 eps:0.137\n",
            "Episode#18000 reward:-200.0 best_reward:-103.0 eps:0.132\n",
            "Episode#18100 reward:-160.0 best_reward:-103.0 eps:0.128\n",
            "Episode#18200 reward:-172.0 best_reward:-103.0 eps:0.124\n",
            "Episode#18300 reward:-189.0 best_reward:-103.0 eps:0.120\n",
            "Episode#18400 reward:-154.0 best_reward:-103.0 eps:0.115\n",
            "Episode#18500 reward:-152.0 best_reward:-103.0 eps:0.111\n",
            "Episode#18600 reward:-200.0 best_reward:-103.0 eps:0.107\n",
            "Episode#18700 reward:-158.0 best_reward:-103.0 eps:0.103\n",
            "Episode#18800 reward:-160.0 best_reward:-103.0 eps:0.099\n",
            "Episode#18900 reward:-200.0 best_reward:-103.0 eps:0.095\n",
            "Episode#19000 reward:-155.0 best_reward:-103.0 eps:0.091\n",
            "Episode#19100 reward:-164.0 best_reward:-103.0 eps:0.087\n",
            "Episode#19200 reward:-156.0 best_reward:-103.0 eps:0.083\n",
            "Episode#19300 reward:-114.0 best_reward:-103.0 eps:0.079\n",
            "Episode#19400 reward:-200.0 best_reward:-103.0 eps:0.075\n",
            "Episode#19500 reward:-178.0 best_reward:-103.0 eps:0.071\n",
            "Episode#19600 reward:-159.0 best_reward:-103.0 eps:0.067\n",
            "Episode#19700 reward:-199.0 best_reward:-103.0 eps:0.062\n",
            "Episode#19800 reward:-155.0 best_reward:-103.0 eps:0.058\n",
            "Episode#19900 reward:-135.0 best_reward:-103.0 eps:0.054\n",
            "Episode#20000 reward:-170.0 best_reward:-103.0 eps:0.050\n",
            "Episode#20100 reward:-153.0 best_reward:-103.0 eps:0.046\n",
            "Episode#20200 reward:-151.0 best_reward:-103.0 eps:0.042\n",
            "Episode#20300 reward:-122.0 best_reward:-103.0 eps:0.039\n",
            "Episode#20400 reward:-164.0 best_reward:-103.0 eps:0.035\n",
            "Episode#20500 reward:-152.0 best_reward:-103.0 eps:0.031\n",
            "Episode#20600 reward:-119.0 best_reward:-103.0 eps:0.028\n",
            "Episode#20700 reward:-150.0 best_reward:-103.0 eps:0.024\n",
            "Episode#20800 reward:-121.0 best_reward:-103.0 eps:0.020\n",
            "Episode#20900 reward:-158.0 best_reward:-103.0 eps:0.016\n",
            "Episode#21000 reward:-149.0 best_reward:-103.0 eps:0.012\n",
            "Episode#21100 reward:-200.0 best_reward:-103.0 eps:0.008\n",
            "Episode#21200 reward:-200.0 best_reward:-103.0 eps:0.005\n",
            "Episode#21300 reward:-145.0 best_reward:-103.0 eps:0.005\n",
            "Episode#21400 reward:-200.0 best_reward:-103.0 eps:0.005\n",
            "Episode#21500 reward:-176.0 best_reward:-103.0 eps:0.005\n",
            "Episode#21600 reward:-193.0 best_reward:-103.0 eps:0.005\n",
            "Episode#21700 reward:-161.0 best_reward:-100.0 eps:0.005\n",
            "Episode#21800 reward:-152.0 best_reward:-100.0 eps:0.005\n",
            "Episode#21900 reward:-200.0 best_reward:-100.0 eps:0.005\n",
            "Episode#22000 reward:-169.0 best_reward:-100.0 eps:0.005\n",
            "Episode#22100 reward:-186.0 best_reward:-100.0 eps:0.005\n",
            "Episode#22200 reward:-152.0 best_reward:-100.0 eps:0.005\n",
            "Episode#22300 reward:-145.0 best_reward:-100.0 eps:0.005\n",
            "Episode#22400 reward:-145.0 best_reward:-100.0 eps:0.005\n",
            "Episode#22500 reward:-143.0 best_reward:-100.0 eps:0.005\n",
            "Episode#22600 reward:-150.0 best_reward:-100.0 eps:0.005\n",
            "Episode#22700 reward:-158.0 best_reward:-100.0 eps:0.005\n",
            "Episode#22800 reward:-200.0 best_reward:-100.0 eps:0.005\n",
            "Episode#22900 reward:-135.0 best_reward:-100.0 eps:0.005\n",
            "Episode#23000 reward:-119.0 best_reward:-100.0 eps:0.005\n",
            "Episode#23100 reward:-153.0 best_reward:-100.0 eps:0.005\n",
            "Episode#23200 reward:-152.0 best_reward:-100.0 eps:0.005\n",
            "Episode#23300 reward:-138.0 best_reward:-100.0 eps:0.005\n",
            "Episode#23400 reward:-120.0 best_reward:-100.0 eps:0.005\n",
            "Episode#23500 reward:-114.0 best_reward:-100.0 eps:0.005\n",
            "Episode#23600 reward:-153.0 best_reward:-100.0 eps:0.005\n",
            "Episode#23700 reward:-154.0 best_reward:-100.0 eps:0.005\n",
            "Episode#23800 reward:-150.0 best_reward:-100.0 eps:0.005\n",
            "Episode#23900 reward:-156.0 best_reward:-100.0 eps:0.005\n",
            "Episode#24000 reward:-153.0 best_reward:-100.0 eps:0.005\n",
            "Episode#24100 reward:-151.0 best_reward:-100.0 eps:0.005\n",
            "Episode#24200 reward:-155.0 best_reward:-100.0 eps:0.005\n",
            "Episode#24300 reward:-189.0 best_reward:-100.0 eps:0.005\n",
            "Episode#24400 reward:-190.0 best_reward:-100.0 eps:0.005\n",
            "Episode#24500 reward:-191.0 best_reward:-100.0 eps:0.005\n",
            "Episode#24600 reward:-161.0 best_reward:-100.0 eps:0.005\n",
            "Episode#24700 reward:-159.0 best_reward:-100.0 eps:0.005\n",
            "Episode#24800 reward:-156.0 best_reward:-100.0 eps:0.005\n",
            "Episode#24900 reward:-147.0 best_reward:-100.0 eps:0.005\n",
            "Episode#25000 reward:-151.0 best_reward:-100.0 eps:0.005\n",
            "Episode#25100 reward:-147.0 best_reward:-100.0 eps:0.005\n",
            "Episode#25200 reward:-112.0 best_reward:-100.0 eps:0.005\n",
            "Episode#25300 reward:-154.0 best_reward:-100.0 eps:0.005\n",
            "Episode#25400 reward:-114.0 best_reward:-100.0 eps:0.005\n",
            "Episode#25500 reward:-144.0 best_reward:-100.0 eps:0.005\n",
            "Episode#25600 reward:-131.0 best_reward:-100.0 eps:0.005\n",
            "Episode#25700 reward:-200.0 best_reward:-100.0 eps:0.005\n",
            "Episode#25800 reward:-145.0 best_reward:-100.0 eps:0.005\n",
            "Episode#25900 reward:-151.0 best_reward:-100.0 eps:0.005\n",
            "Episode#26000 reward:-149.0 best_reward:-100.0 eps:0.005\n",
            "Episode#26100 reward:-143.0 best_reward:-100.0 eps:0.005\n",
            "Episode#26200 reward:-156.0 best_reward:-100.0 eps:0.005\n",
            "Episode#26300 reward:-113.0 best_reward:-100.0 eps:0.005\n",
            "Episode#26400 reward:-139.0 best_reward:-100.0 eps:0.005\n",
            "Episode#26500 reward:-117.0 best_reward:-100.0 eps:0.005\n",
            "Episode#26600 reward:-139.0 best_reward:-100.0 eps:0.005\n",
            "Episode#26700 reward:-184.0 best_reward:-100.0 eps:0.005\n",
            "Episode#26800 reward:-196.0 best_reward:-100.0 eps:0.005\n",
            "Episode#26900 reward:-149.0 best_reward:-100.0 eps:0.005\n",
            "Episode#27000 reward:-158.0 best_reward:-100.0 eps:0.005\n",
            "Episode#27100 reward:-162.0 best_reward:-100.0 eps:0.005\n",
            "Episode#27200 reward:-140.0 best_reward:-100.0 eps:0.005\n",
            "Episode#27300 reward:-154.0 best_reward:-100.0 eps:0.005\n",
            "Episode#27400 reward:-159.0 best_reward:-100.0 eps:0.005\n",
            "Episode#27500 reward:-112.0 best_reward:-100.0 eps:0.005\n",
            "Episode#27600 reward:-155.0 best_reward:-100.0 eps:0.005\n",
            "Episode#27700 reward:-161.0 best_reward:-100.0 eps:0.005\n",
            "Episode#27800 reward:-147.0 best_reward:-100.0 eps:0.005\n",
            "Episode#27900 reward:-153.0 best_reward:-100.0 eps:0.005\n",
            "Episode#28000 reward:-159.0 best_reward:-100.0 eps:0.005\n",
            "Episode#28100 reward:-143.0 best_reward:-100.0 eps:0.005\n",
            "Episode#28200 reward:-144.0 best_reward:-100.0 eps:0.005\n",
            "Episode#28300 reward:-155.0 best_reward:-100.0 eps:0.005\n",
            "Episode#28400 reward:-193.0 best_reward:-100.0 eps:0.005\n",
            "Episode#28500 reward:-144.0 best_reward:-100.0 eps:0.005\n",
            "Episode#28600 reward:-153.0 best_reward:-100.0 eps:0.005\n",
            "Episode#28700 reward:-117.0 best_reward:-95.0 eps:0.005\n",
            "Episode#28800 reward:-143.0 best_reward:-95.0 eps:0.005\n",
            "Episode#28900 reward:-144.0 best_reward:-95.0 eps:0.005\n",
            "Episode#29000 reward:-159.0 best_reward:-95.0 eps:0.005\n",
            "Episode#29100 reward:-140.0 best_reward:-95.0 eps:0.005\n",
            "Episode#29200 reward:-124.0 best_reward:-95.0 eps:0.005\n",
            "Episode#29300 reward:-153.0 best_reward:-95.0 eps:0.005\n",
            "Episode#29400 reward:-151.0 best_reward:-95.0 eps:0.005\n",
            "Episode#29500 reward:-134.0 best_reward:-95.0 eps:0.005\n",
            "Episode#29600 reward:-143.0 best_reward:-95.0 eps:0.005\n",
            "Episode#29700 reward:-109.0 best_reward:-95.0 eps:0.005\n",
            "Episode#29800 reward:-112.0 best_reward:-95.0 eps:0.005\n",
            "Episode#29900 reward:-109.0 best_reward:-95.0 eps:0.005\n",
            "Episode#30000 reward:-152.0 best_reward:-95.0 eps:0.005\n",
            "Episode#30100 reward:-106.0 best_reward:-95.0 eps:0.005\n",
            "Episode#30200 reward:-143.0 best_reward:-95.0 eps:0.005\n",
            "Episode#30300 reward:-153.0 best_reward:-95.0 eps:0.005\n",
            "Episode#30400 reward:-143.0 best_reward:-95.0 eps:0.005\n",
            "Episode#30500 reward:-150.0 best_reward:-95.0 eps:0.005\n",
            "Episode#30600 reward:-116.0 best_reward:-95.0 eps:0.005\n",
            "Episode#30700 reward:-164.0 best_reward:-95.0 eps:0.005\n",
            "Episode#30800 reward:-177.0 best_reward:-94.0 eps:0.005\n",
            "Episode#30900 reward:-152.0 best_reward:-94.0 eps:0.005\n",
            "Episode#31000 reward:-164.0 best_reward:-94.0 eps:0.005\n",
            "Episode#31100 reward:-200.0 best_reward:-94.0 eps:0.005\n",
            "Episode#31200 reward:-154.0 best_reward:-94.0 eps:0.005\n",
            "Episode#31300 reward:-138.0 best_reward:-94.0 eps:0.005\n",
            "Episode#31400 reward:-160.0 best_reward:-90.0 eps:0.005\n",
            "Episode#31500 reward:-165.0 best_reward:-90.0 eps:0.005\n",
            "Episode#31600 reward:-172.0 best_reward:-90.0 eps:0.005\n",
            "Episode#31700 reward:-115.0 best_reward:-90.0 eps:0.005\n",
            "Episode#31800 reward:-144.0 best_reward:-90.0 eps:0.005\n",
            "Episode#31900 reward:-143.0 best_reward:-90.0 eps:0.005\n",
            "Episode#32000 reward:-145.0 best_reward:-90.0 eps:0.005\n",
            "Episode#32100 reward:-145.0 best_reward:-90.0 eps:0.005\n",
            "Episode#32200 reward:-150.0 best_reward:-90.0 eps:0.005\n",
            "Episode#32300 reward:-128.0 best_reward:-90.0 eps:0.005\n",
            "Episode#32400 reward:-135.0 best_reward:-90.0 eps:0.005\n",
            "Episode#32500 reward:-150.0 best_reward:-90.0 eps:0.005\n",
            "Episode#32600 reward:-149.0 best_reward:-90.0 eps:0.005\n",
            "Episode#32700 reward:-149.0 best_reward:-90.0 eps:0.005\n",
            "Episode#32800 reward:-114.0 best_reward:-90.0 eps:0.005\n",
            "Episode#32900 reward:-146.0 best_reward:-90.0 eps:0.005\n",
            "Episode#33000 reward:-119.0 best_reward:-90.0 eps:0.005\n",
            "Episode#33100 reward:-142.0 best_reward:-90.0 eps:0.005\n",
            "Episode#33200 reward:-113.0 best_reward:-90.0 eps:0.005\n",
            "Episode#33300 reward:-150.0 best_reward:-90.0 eps:0.005\n",
            "Episode#33400 reward:-154.0 best_reward:-90.0 eps:0.005\n",
            "Episode#33500 reward:-111.0 best_reward:-90.0 eps:0.005\n",
            "Episode#33600 reward:-145.0 best_reward:-90.0 eps:0.005\n",
            "Episode#33700 reward:-113.0 best_reward:-90.0 eps:0.005\n",
            "Episode#33800 reward:-141.0 best_reward:-90.0 eps:0.005\n",
            "Episode#33900 reward:-141.0 best_reward:-90.0 eps:0.005\n",
            "Episode#34000 reward:-149.0 best_reward:-90.0 eps:0.005\n",
            "Episode#34100 reward:-113.0 best_reward:-90.0 eps:0.005\n",
            "Episode#34200 reward:-112.0 best_reward:-90.0 eps:0.005\n",
            "Episode#34300 reward:-143.0 best_reward:-90.0 eps:0.005\n",
            "Episode#34400 reward:-119.0 best_reward:-90.0 eps:0.005\n",
            "Episode#34500 reward:-118.0 best_reward:-90.0 eps:0.005\n",
            "Episode#34600 reward:-160.0 best_reward:-90.0 eps:0.005\n",
            "Episode#34700 reward:-164.0 best_reward:-87.0 eps:0.005\n",
            "Episode#34800 reward:-129.0 best_reward:-87.0 eps:0.005\n",
            "Episode#34900 reward:-119.0 best_reward:-87.0 eps:0.005\n",
            "Episode#35000 reward:-162.0 best_reward:-87.0 eps:0.005\n",
            "Episode#35100 reward:-169.0 best_reward:-87.0 eps:0.005\n",
            "Episode#35200 reward:-111.0 best_reward:-87.0 eps:0.005\n",
            "Episode#35300 reward:-156.0 best_reward:-85.0 eps:0.005\n",
            "Episode#35400 reward:-139.0 best_reward:-85.0 eps:0.005\n",
            "Episode#35500 reward:-164.0 best_reward:-85.0 eps:0.005\n",
            "Episode#35600 reward:-87.0 best_reward:-85.0 eps:0.005\n",
            "Episode#35700 reward:-116.0 best_reward:-85.0 eps:0.005\n",
            "Episode#35800 reward:-150.0 best_reward:-85.0 eps:0.005\n",
            "Episode#35900 reward:-87.0 best_reward:-85.0 eps:0.005\n",
            "Episode#36000 reward:-161.0 best_reward:-85.0 eps:0.005\n",
            "Episode#36100 reward:-163.0 best_reward:-85.0 eps:0.005\n",
            "Episode#36200 reward:-146.0 best_reward:-85.0 eps:0.005\n",
            "Episode#36300 reward:-200.0 best_reward:-85.0 eps:0.005\n",
            "Episode#36400 reward:-174.0 best_reward:-85.0 eps:0.005\n",
            "Episode#36500 reward:-136.0 best_reward:-85.0 eps:0.005\n",
            "Episode#36600 reward:-152.0 best_reward:-85.0 eps:0.005\n",
            "Episode#36700 reward:-153.0 best_reward:-85.0 eps:0.005\n",
            "Episode#36800 reward:-141.0 best_reward:-85.0 eps:0.005\n",
            "Episode#36900 reward:-157.0 best_reward:-85.0 eps:0.005\n",
            "Episode#37000 reward:-197.0 best_reward:-85.0 eps:0.005\n",
            "Episode#37100 reward:-172.0 best_reward:-85.0 eps:0.005\n",
            "Episode#37200 reward:-147.0 best_reward:-85.0 eps:0.005\n",
            "Episode#37300 reward:-135.0 best_reward:-85.0 eps:0.005\n",
            "Episode#37400 reward:-117.0 best_reward:-85.0 eps:0.005\n",
            "Episode#37500 reward:-152.0 best_reward:-85.0 eps:0.005\n",
            "Episode#37600 reward:-120.0 best_reward:-85.0 eps:0.005\n",
            "Episode#37700 reward:-114.0 best_reward:-85.0 eps:0.005\n",
            "Episode#37800 reward:-148.0 best_reward:-85.0 eps:0.005\n",
            "Episode#37900 reward:-149.0 best_reward:-85.0 eps:0.005\n",
            "Episode#38000 reward:-140.0 best_reward:-85.0 eps:0.005\n",
            "Episode#38100 reward:-142.0 best_reward:-85.0 eps:0.005\n",
            "Episode#38200 reward:-140.0 best_reward:-85.0 eps:0.005\n",
            "Episode#38300 reward:-139.0 best_reward:-85.0 eps:0.005\n",
            "Episode#38400 reward:-141.0 best_reward:-85.0 eps:0.005\n",
            "Episode#38500 reward:-109.0 best_reward:-85.0 eps:0.005\n",
            "Episode#38600 reward:-186.0 best_reward:-85.0 eps:0.005\n",
            "Episode#38700 reward:-190.0 best_reward:-85.0 eps:0.005\n",
            "Episode#38800 reward:-139.0 best_reward:-85.0 eps:0.005\n",
            "Episode#38900 reward:-200.0 best_reward:-85.0 eps:0.005\n",
            "Episode#39000 reward:-200.0 best_reward:-85.0 eps:0.005\n",
            "Episode#39100 reward:-162.0 best_reward:-85.0 eps:0.005\n",
            "Episode#39200 reward:-193.0 best_reward:-85.0 eps:0.005\n",
            "Episode#39300 reward:-110.0 best_reward:-85.0 eps:0.005\n",
            "Episode#39400 reward:-118.0 best_reward:-85.0 eps:0.005\n",
            "Episode#39500 reward:-114.0 best_reward:-85.0 eps:0.005\n",
            "Episode#39600 reward:-125.0 best_reward:-85.0 eps:0.005\n",
            "Episode#39700 reward:-166.0 best_reward:-85.0 eps:0.005\n",
            "Episode#39800 reward:-137.0 best_reward:-85.0 eps:0.005\n",
            "Episode#39900 reward:-138.0 best_reward:-85.0 eps:0.005\n",
            "Episode#40000 reward:-160.0 best_reward:-85.0 eps:0.005\n",
            "Episode#40100 reward:-149.0 best_reward:-85.0 eps:0.005\n",
            "Episode#40200 reward:-109.0 best_reward:-85.0 eps:0.005\n",
            "Episode#40300 reward:-142.0 best_reward:-85.0 eps:0.005\n",
            "Episode#40400 reward:-133.0 best_reward:-85.0 eps:0.005\n",
            "Episode#40500 reward:-149.0 best_reward:-85.0 eps:0.005\n",
            "Episode#40600 reward:-108.0 best_reward:-85.0 eps:0.005\n",
            "Episode#40700 reward:-142.0 best_reward:-85.0 eps:0.005\n",
            "Episode#40800 reward:-143.0 best_reward:-85.0 eps:0.005\n",
            "Episode#40900 reward:-173.0 best_reward:-85.0 eps:0.005\n",
            "Episode#41000 reward:-144.0 best_reward:-85.0 eps:0.005\n",
            "Episode#41100 reward:-144.0 best_reward:-85.0 eps:0.005\n",
            "Episode#41200 reward:-118.0 best_reward:-85.0 eps:0.005\n",
            "Episode#41300 reward:-135.0 best_reward:-85.0 eps:0.005\n",
            "Episode#41400 reward:-134.0 best_reward:-85.0 eps:0.005\n",
            "Episode#41500 reward:-135.0 best_reward:-85.0 eps:0.005\n",
            "Episode#41600 reward:-137.0 best_reward:-85.0 eps:0.005\n",
            "Episode#41700 reward:-143.0 best_reward:-85.0 eps:0.005\n",
            "Episode#41800 reward:-154.0 best_reward:-85.0 eps:0.005\n",
            "Episode#41900 reward:-147.0 best_reward:-85.0 eps:0.005\n",
            "Episode#42000 reward:-112.0 best_reward:-85.0 eps:0.005\n",
            "Episode#42100 reward:-143.0 best_reward:-85.0 eps:0.005\n",
            "Episode#42200 reward:-143.0 best_reward:-85.0 eps:0.005\n",
            "Episode#42300 reward:-116.0 best_reward:-85.0 eps:0.005\n",
            "Episode#42400 reward:-111.0 best_reward:-85.0 eps:0.005\n",
            "Episode#42500 reward:-152.0 best_reward:-85.0 eps:0.005\n",
            "Episode#42600 reward:-148.0 best_reward:-85.0 eps:0.005\n",
            "Episode#42700 reward:-111.0 best_reward:-85.0 eps:0.005\n",
            "Episode#42800 reward:-152.0 best_reward:-85.0 eps:0.005\n",
            "Episode#42900 reward:-113.0 best_reward:-85.0 eps:0.005\n",
            "Episode#43000 reward:-143.0 best_reward:-85.0 eps:0.005\n",
            "Episode#43100 reward:-118.0 best_reward:-85.0 eps:0.005\n",
            "Episode#43200 reward:-115.0 best_reward:-85.0 eps:0.005\n",
            "Episode#43300 reward:-114.0 best_reward:-85.0 eps:0.005\n",
            "Episode#43400 reward:-117.0 best_reward:-85.0 eps:0.005\n",
            "Episode#43500 reward:-116.0 best_reward:-85.0 eps:0.005\n",
            "Episode#43600 reward:-116.0 best_reward:-85.0 eps:0.005\n",
            "Episode#43700 reward:-115.0 best_reward:-85.0 eps:0.005\n",
            "Episode#43800 reward:-116.0 best_reward:-85.0 eps:0.005\n",
            "Episode#43900 reward:-154.0 best_reward:-85.0 eps:0.005\n",
            "Episode#44000 reward:-153.0 best_reward:-85.0 eps:0.005\n",
            "Episode#44100 reward:-116.0 best_reward:-85.0 eps:0.005\n",
            "Episode#44200 reward:-114.0 best_reward:-85.0 eps:0.005\n",
            "Episode#44300 reward:-136.0 best_reward:-85.0 eps:0.005\n",
            "Episode#44400 reward:-116.0 best_reward:-85.0 eps:0.005\n",
            "Episode#44500 reward:-139.0 best_reward:-85.0 eps:0.005\n",
            "Episode#44600 reward:-151.0 best_reward:-85.0 eps:0.005\n",
            "Episode#44700 reward:-114.0 best_reward:-85.0 eps:0.005\n",
            "Episode#44800 reward:-114.0 best_reward:-85.0 eps:0.005\n",
            "Episode#44900 reward:-145.0 best_reward:-85.0 eps:0.005\n",
            "Episode#45000 reward:-111.0 best_reward:-85.0 eps:0.005\n",
            "Episode#45100 reward:-112.0 best_reward:-85.0 eps:0.005\n",
            "Episode#45200 reward:-144.0 best_reward:-85.0 eps:0.005\n",
            "Episode#45300 reward:-111.0 best_reward:-85.0 eps:0.005\n",
            "Episode#45400 reward:-137.0 best_reward:-85.0 eps:0.005\n",
            "Episode#45500 reward:-112.0 best_reward:-85.0 eps:0.005\n",
            "Episode#45600 reward:-113.0 best_reward:-85.0 eps:0.005\n",
            "Episode#45700 reward:-114.0 best_reward:-85.0 eps:0.005\n",
            "Episode#45800 reward:-158.0 best_reward:-85.0 eps:0.005\n",
            "Episode#45900 reward:-154.0 best_reward:-85.0 eps:0.005\n",
            "Episode#46000 reward:-113.0 best_reward:-85.0 eps:0.005\n",
            "Episode#46100 reward:-146.0 best_reward:-85.0 eps:0.005\n",
            "Episode#46200 reward:-116.0 best_reward:-85.0 eps:0.005\n",
            "Episode#46300 reward:-114.0 best_reward:-85.0 eps:0.005\n",
            "Episode#46400 reward:-117.0 best_reward:-85.0 eps:0.005\n",
            "Episode#46500 reward:-115.0 best_reward:-85.0 eps:0.005\n",
            "Episode#46600 reward:-117.0 best_reward:-85.0 eps:0.005\n",
            "Episode#46700 reward:-168.0 best_reward:-85.0 eps:0.005\n",
            "Episode#46800 reward:-152.0 best_reward:-85.0 eps:0.005\n",
            "Episode#46900 reward:-108.0 best_reward:-85.0 eps:0.005\n",
            "Episode#47000 reward:-110.0 best_reward:-85.0 eps:0.005\n",
            "Episode#47100 reward:-115.0 best_reward:-85.0 eps:0.005\n",
            "Episode#47200 reward:-148.0 best_reward:-85.0 eps:0.005\n",
            "Episode#47300 reward:-142.0 best_reward:-85.0 eps:0.005\n",
            "Episode#47400 reward:-109.0 best_reward:-85.0 eps:0.005\n",
            "Episode#47500 reward:-160.0 best_reward:-85.0 eps:0.005\n",
            "Episode#47600 reward:-152.0 best_reward:-85.0 eps:0.005\n",
            "Episode#47700 reward:-151.0 best_reward:-85.0 eps:0.005\n",
            "Episode#47800 reward:-150.0 best_reward:-85.0 eps:0.005\n",
            "Episode#47900 reward:-109.0 best_reward:-85.0 eps:0.005\n",
            "Episode#48000 reward:-109.0 best_reward:-85.0 eps:0.005\n",
            "Episode#48100 reward:-113.0 best_reward:-85.0 eps:0.005\n",
            "Episode#48200 reward:-112.0 best_reward:-85.0 eps:0.005\n",
            "Episode#48300 reward:-109.0 best_reward:-85.0 eps:0.005\n",
            "Episode#48400 reward:-168.0 best_reward:-85.0 eps:0.005\n",
            "Episode#48500 reward:-108.0 best_reward:-85.0 eps:0.005\n",
            "Episode#48600 reward:-197.0 best_reward:-85.0 eps:0.005\n",
            "Episode#48700 reward:-106.0 best_reward:-85.0 eps:0.005\n",
            "Episode#48800 reward:-108.0 best_reward:-85.0 eps:0.005\n",
            "Episode#48900 reward:-108.0 best_reward:-85.0 eps:0.005\n",
            "Episode#49000 reward:-118.0 best_reward:-85.0 eps:0.005\n",
            "Episode#49100 reward:-118.0 best_reward:-85.0 eps:0.005\n",
            "Episode#49200 reward:-108.0 best_reward:-85.0 eps:0.005\n",
            "Episode#49300 reward:-120.0 best_reward:-85.0 eps:0.005\n",
            "Episode#49400 reward:-129.0 best_reward:-85.0 eps:0.005\n",
            "Episode#49500 reward:-118.0 best_reward:-85.0 eps:0.005\n",
            "Episode#49600 reward:-155.0 best_reward:-85.0 eps:0.005\n",
            "Episode#49700 reward:-115.0 best_reward:-85.0 eps:0.005\n",
            "Episode#49800 reward:-151.0 best_reward:-85.0 eps:0.005\n",
            "Episode#49900 reward:-118.0 best_reward:-85.0 eps:0.005\n",
            "Moviepy - Building video /content/videos/rl-video-episode-0.mp4.\n",
            "Moviepy - Writing video /content/videos/rl-video-episode-0.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/rl-video-episode-0.mp4\n",
            "Test episode reward: -111.0\n",
            "Moviepy - Building video /content/videos/rl-video-episode-1.mp4.\n",
            "Moviepy - Writing video /content/videos/rl-video-episode-1.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/rl-video-episode-1.mp4\n",
            "Test episode reward: -110.0\n",
            "Test episode reward: -100.0\n",
            "Test episode reward: -148.0\n",
            "Test episode reward: -147.0\n",
            "Test episode reward: -111.0\n",
            "Test episode reward: -109.0\n",
            "Test episode reward: -111.0\n",
            "Moviepy - Building video /content/videos/rl-video-episode-8.mp4.\n",
            "Moviepy - Writing video /content/videos/rl-video-episode-8.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                               "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/rl-video-episode-8.mp4\n",
            "Test episode reward: -111.0\n",
            "Test episode reward: -111.0\n",
            "Test episode reward: -97.0\n",
            "Test episode reward: -100.0\n",
            "Test episode reward: -111.0\n",
            "Test episode reward: -112.0\n",
            "Test episode reward: -111.0\n",
            "Test episode reward: -156.0\n",
            "Test episode reward: -143.0\n",
            "Test episode reward: -100.0\n",
            "Test episode reward: -153.0\n",
            "Test episode reward: -112.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "\n",
        "In this project, two reinforcement learning algorithms, Q-Learning and SARSA, were applied to solve the Mountain Car problem. The results demonstrate distinct learning patterns and final performances for each method, illustrating their unique strengths and weaknesses."
      ],
      "metadata": {
        "id": "mbjomA2ZpkaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### Q-Learning vs. SARSA Performance\n",
        "\n",
        "1. **Learning Behavior:**\n",
        "   - **Q-Learning:** In the early stages, Q-Learning exhibited faster learning due to its off-policy nature, which allows the agent to maximize long-term rewards by considering actions outside the current policy. This led to more aggressive exploration, yielding higher episodic rewards earlier in the training.\n",
        "   - **SARSA:** On the other hand, SARSA, being an on-policy method, tended to be more conservative in its exploration. This approach made the learning process more stable but resulted in slower initial progress compared to Q-Learning. SARSA gradually improved its performance as the number of episodes increased.\n",
        "\n",
        "2. **Convergence and Stability:**\n",
        "   - **Q-Learning** displayed more fluctuation in rewards as it focused on maximizing future rewards even at the risk of taking suboptimal actions during training. Despite these fluctuations, Q-Learning converged faster in terms of finding a solution that allowed the car to reach the top of the hill.\n",
        "   - **SARSA** had a smoother learning curve with fewer fluctuations, showing a more stable convergence pattern. This is attributed to SARSA's conservative nature, which ensures it sticks closely to the policy being evaluated, reducing the chances of drastic changes in learned behavior.\n",
        "\n",
        "3. **Exploration and Exploitation:**\n",
        "   - The exploration rate (epsilon) decreased steadily in both methods, indicating that both algorithms reduced exploration over time as the policy became more stable. By the end of the training, both Q-Learning and SARSA were operating with minimal exploration, focusing on exploiting the learned policy.\n"
      ],
      "metadata": {
        "id": "Ujkw0Ijxo8IH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Conclusion\n",
        "\n",
        "The results show that Q-Learning outperforms SARSA in terms of speed of convergence due to its off-policy nature, which allows for more aggressive learning. However, SARSA's on-policy approach results in a more stable learning process, which may be preferable in environments where stability is critical. Overall, the choice between Q-Learning and SARSA should be guided by the specific requirements of the problem at hand, such as the need for speed or stability in learning.\n",
        "\n",
        "This analysis highlights the trade-offs between these two algorithms, providing insights into their behavior and suitability for solving reinforcement learning problems."
      ],
      "metadata": {
        "id": "FcK7D6x-peT8"
      }
    }
  ]
}